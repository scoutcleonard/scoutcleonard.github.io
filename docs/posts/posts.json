[
  {
    "path": "posts/2022-03-22-spatial-analysis-protecting-whales/",
    "title": "Spatial Analysis: Protecting Whales",
    "description": "Using Python, I generate a whale-protecting reduced speeding zone and calculate the impact on shipping for the coast of Dominica.",
    "author": [
      {
        "name": "Scout Leonard",
        "url": {
          "https://scoutcleonard.github.io": {}
        }
      }
    ],
    "date": "2022-03-22",
    "categories": [
      "class projects"
    ],
    "contents": "\nIntro\nThis blog post is based on an assignment from my Master’s of Environmental Data Science course EDS 223: Spatial Analysis for Environmental Data Science.\nLoad Libraries\n#load libraries \nimport pandas as pd\nimport geopandas as gpd\nimport matplotlib.pyplot as plt\nimport os\nimport numpy as np\nfrom shapely.geometry import Polygon\nDominica Outline\nIn the code chunk below, I define a file path for Dominica spatial data:\ninput_folder = r\"data/dominica\"\ndominica_shapefile = \"dma_admn_adm0_py_s1_dominode_v2.shp\"\n\ndominica_fp = os.path.join(input_folder, dominica_shapefile)\nNext, I read in the Dominica shapefile using read_file():\ndominica_data = gpd.read_file(dominica_fp)\nThen I change the CRS from WGS 84 to Dominica 1945 using the function to_crs(). This transforms the CRS to a more accurate geospatial reference system for the region of interest, coastal Dominica:\ndominica_2002 = dominica_data.to_crs(\"EPSG:2002\")\nNext, I plot the outline of Dominica to explore and check that the output is expected:\nfig, ax = plt.subplots(figsize=(3, 3), dpi=200)\nax.grid(True)\nax.set_facecolor('lightblue')\n\ndominica_2002.plot(ax = ax, color = 'green')\nWhale Sighting Data\nNext, I define the file path for whale sighting spatial data:\nsightings_input_folder = r\"data\"\nsightings_csv = \"sightings2005_2018.csv\"\n\nsightings_fp = os.path.join(sightings_input_folder, sightings_csv)\nRead file using read_file():\nsightings_data = gpd.read_file(sightings_fp)\nBootstrap the geometries:\nsightings_points = gpd.points_from_xy(sightings_data['Long'], sightings_data['Lat'])\nProject the dataset into a Geo Data Frame with the appropriate CRS:\nsightings_gdf_4326 = gpd.GeoDataFrame(sightings_data, geometry=sightings_points, crs = 'EPSG:4326')\nsightings_gdf_2002 = sightings_gdf_4326.to_crs(\"EPSG:2002\")\nCreate Grid\nDefine the grid’s boundaries:\nminx, miny, maxx, maxy = sightings_gdf_2002.total_bounds\nDefine the grid’s cell size, in meters:\ncell_size = 2000\nxs = np.arange(minx, maxx, cell_size)\nys = np.arange(miny, maxy, cell_size)\ndef make_cell(x, y, cell_size):\n    ring = [\n        (x, y),\n        (x + cell_size, y),\n        (x + cell_size, y + cell_size),\n        (x, y + cell_size)\n    ]\n    cell = Polygon(ring)\n    return cell\ncells = []\nfor x in xs:\n    for y in ys:\n        cell = make_cell(x, y, cell_size)\n        cells.append(cell)\nCreate a grid GeoDataFrame, containing a cell for each row:\ngrid = gpd.GeoDataFrame({'geometry': cells}, crs=2002)\nPlot the grid to see how it looks:\nWe adjust the figure size so the cells are more visible than in the default grid:\ngrid.boundary.plot(figsize = (10,20))\n<AxesSubplot:>\npngExtract Whale Habitat\nSpatially join whale sightings data with grid data to get whale counts per grid cell:\nsightings_with_grid = grid.sjoin(sightings_gdf_2002, how=\"inner\")\nsightings_with_grid\n\n\n\n\ngeometry\n\n\nindex_right\n\n\nfield_1\n\n\nGPStime\n\n\nLat\n\n\nLong\n\n\n124\n\n\nPOLYGON ((408480.652 1780792.746, 410480.652 1…\n\n\n4327\n\n\n4327\n\n\n2018-03-15 06:41:03\n\n\n16.127\n\n\n-61.896866\n\n\n124\n\n\nPOLYGON ((408480.652 1780792.746, 410480.652 1…\n\n\n4328\n\n\n4328\n\n\n2018-03-15 06:44:05\n\n\n16.127666\n\n\n-61.900766\n\n\n124\n\n\nPOLYGON ((408480.652 1780792.746, 410480.652 1…\n\n\n4329\n\n\n4329\n\n\n2018-03-15 06:58:17\n\n\n16.1305\n\n\n-61.903366\n\n\n125\n\n\nPOLYGON ((408480.652 1782792.746, 410480.652 1…\n\n\n4330\n\n\n4330\n\n\n2018-03-15 07:15:33\n\n\n16.139583\n\n\n-61.900116\n\n\n125\n\n\nPOLYGON ((408480.652 1782792.746, 410480.652 1…\n\n\n4331\n\n\n4331\n\n\n2018-03-15 07:17:30\n\n\n16.14175\n\n\n-61.897716\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n5133\n\n\nPOLYGON ((484480.652 1690792.746, 486480.652 1…\n\n\n1147\n\n\n1147\n\n\n2008-05-04 16:59:36\n\n\n15.304085\n\n\n-61.194134\n\n\n5134\n\n\nPOLYGON ((484480.652 1692792.746, 486480.652 1…\n\n\n1148\n\n\n1148\n\n\n2008-05-04 17:43:45\n\n\n15.321439\n\n\n-61.19188\n\n\n5985\n\n\nPOLYGON ((498480.652 1532792.746, 500480.652 1…\n\n\n609\n\n\n609\n\n\n2005-03-20 11:50:05\n\n\n13.86967067\n\n\n-61.0794355\n\n\n5985\n\n\nPOLYGON ((498480.652 1532792.746, 500480.652 1…\n\n\n611\n\n\n611\n\n\n2005-03-20 12:56:58\n\n\n13.86967067\n\n\n-61.0794355\n\n\n5985\n\n\nPOLYGON ((498480.652 1532792.746, 500480.652 1…\n\n\n610\n\n\n610\n\n\n2005-03-20 12:05:10\n\n\n13.86967067\n\n\n-61.0794355\n\n\n4893 rows × 6 columns\n\n\nPlot the Grid Cells with Whale Counts:\nsightings_with_grid.plot(figsize = (10,20))\n<AxesSubplot:>\npngTo ge the count of sightings in each cell, we have to perform a so called map-reduce operation using groupby and count. This is added to the Geo Data Frame as a new column:\ngrid['count'] = sightings_with_grid.groupby(sightings_with_grid.index).count()['index_right']\ngrid\n\n\n\n\ngeometry\n\n\ncount\n\n\n0\n\n\nPOLYGON ((408480.652 1532792.746, 410480.652 1…\n\n\nNaN\n\n\n1\n\n\nPOLYGON ((408480.652 1534792.746, 410480.652 1…\n\n\nNaN\n\n\n2\n\n\nPOLYGON ((408480.652 1536792.746, 410480.652 1…\n\n\nNaN\n\n\n3\n\n\nPOLYGON ((408480.652 1538792.746, 410480.652 1…\n\n\nNaN\n\n\n4\n\n\nPOLYGON ((408480.652 1540792.746, 410480.652 1…\n\n\nNaN\n\n\n…\n\n\n…\n\n\n…\n\n\n6113\n\n\nPOLYGON ((498480.652 1788792.746, 500480.652 1…\n\n\nNaN\n\n\n6114\n\n\nPOLYGON ((498480.652 1790792.746, 500480.652 1…\n\n\nNaN\n\n\n6115\n\n\nPOLYGON ((498480.652 1792792.746, 500480.652 1…\n\n\nNaN\n\n\n6116\n\n\nPOLYGON ((498480.652 1794792.746, 500480.652 1…\n\n\nNaN\n\n\n6117\n\n\nPOLYGON ((498480.652 1796792.746, 500480.652 1…\n\n\nNaN\n\n\n6118 rows × 2 columns\n\n\nSubset grid df to cells with more than 20 sightings:\ngrid_20 = grid[grid['count'] > 20]\ngrid_20_union = grid_20.unary_union\ngrid_20_union\nsvgNext we create a convex hull using the sightings we joined with unary_union to generate the “whale habitat”:\nhabitat_convex_hull = grid_20_union.convex_hull\nhabitat_convex_hull\nsvgConvert the habitat convex hull polygon to a Geo Data Frame:\nconvex_hull_gdf = gpd.GeoDataFrame(crs = 'EPSG:2002', geometry = [habitat_convex_hull])\nax = convex_hull_gdf.plot(color = 'y')\n\ndominica_2002.plot(ax = ax, figsize = (10,20))\n<AxesSubplot:>\npngVessel Data\nLoad Data\nDefine file path for AIS vessel data:\nais_input_folder = r\"data\"\nais_csv = \"station1249.csv\"\n\nais_fp = os.path.join(ais_input_folder, ais_csv)\nRead file using read_file():\nais_data = gpd.read_file(ais_fp)\nBootstrap the geometries:\nais_points = gpd.points_from_xy(ais_data['LON'], ais_data['LAT'])\nais_gdf = gpd.GeoDataFrame(ais_data, geometry=ais_points, crs = 'EPSG:4326')\nProject to the Dominica 1945 CRS:\nais_gdf = ais_gdf.to_crs(\"EPSG:2002\")\nThe character strings in TIMESTAMP are not in a recognized datetime format. We parse those strings here:\npd.to_datetime(ais_gdf['TIMESTAMP'])\n0        2015-05-22 13:53:26\n1        2015-05-22 13:52:57\n2        2015-05-22 13:52:32\n3        2015-05-22 13:52:24\n4        2015-05-22 13:51:23\n                 ...        \n617257   2015-05-21 21:34:59\n617258   2015-05-21 21:34:55\n617259   2015-05-21 21:34:46\n617260   2015-05-21 21:34:46\n617261   2015-05-21 21:34:45\nName: TIMESTAMP, Length: 617262, dtype: datetime64[ns]\nParse strings of the TIMESTAMP column to a recognized datetime format:\nais_gdf['TIMESTAMP'] = pd.to_datetime(ais_gdf['TIMESTAMP'])\nCalculate Distance and Speed\nSpatially subset vessel data using whale habitat data:\nhabitat_ais_gdf = ais_gdf.sjoin(convex_hull_gdf, how = 'inner')\nSort the dataframe by MMSI (Maritime Mobile Service Identity), the vessel’s unique identifier.\nhabitat_ais_sorted = habitat_ais_gdf.sort_values(['MMSI', 'TIMESTAMP'])\nCreate a copy of our dataframe and shift each observation down one row using shift().\nhabitat_ais_copy = habitat_ais_sorted.copy()\nhabitat_ais_copy\n\n\n\n\nfield_1\n\n\nMMSI\n\n\nLON\n\n\nLAT\n\n\nTIMESTAMP\n\n\ngeometry\n\n\nindex_right\n\n\n235025\n\n\n235025\n\n\n203106200\n\n\n-61.40929\n\n\n15.21021\n\n\n2015-02-25 15:32:20\n\n\nPOINT (462476.396 1680935.224)\n\n\n0\n\n\n235018\n\n\n235018\n\n\n203106200\n\n\n-61.41107\n\n\n15.21436\n\n\n2015-02-25 15:34:50\n\n\nPOINT (462283.995 1681393.698)\n\n\n0\n\n\n235000\n\n\n235000\n\n\n203106200\n\n\n-61.41427\n\n\n15.22638\n\n\n2015-02-25 15:42:19\n\n\nPOINT (461936.769 1682722.187)\n\n\n0\n\n\n234989\n\n\n234989\n\n\n203106200\n\n\n-61.41553\n\n\n15.2353\n\n\n2015-02-25 15:47:19\n\n\nPOINT (461798.818 1683708.377)\n\n\n0\n\n\n234984\n\n\n234984\n\n\n203106200\n\n\n-61.41687\n\n\n15.23792\n\n\n2015-02-25 15:49:50\n\n\nPOINT (461654.150 1683997.765)\n\n\n0\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n259103\n\n\n259103\n\n\n983191049\n\n\n-61.38322\n\n\n15.2927\n\n\n2015-02-19 19:50:45\n\n\nPOINT (465250.372 1690066.434)\n\n\n0\n\n\n259094\n\n\n259094\n\n\n983191049\n\n\n-61.38328\n\n\n15.29259\n\n\n2015-02-19 19:55:09\n\n\nPOINT (465243.965 1690054.249)\n\n\n0\n\n\n258954\n\n\n258954\n\n\n983191049\n\n\n-61.38344\n\n\n15.2932\n\n\n2015-02-19 20:51:12\n\n\nPOINT (465226.597 1690121.667)\n\n\n0\n\n\n258930\n\n\n258930\n\n\n983191049\n\n\n-61.38329\n\n\n15.29258\n\n\n2015-02-19 21:02:54\n\n\nPOINT (465242.895 1690053.140)\n\n\n0\n\n\n258206\n\n\n258206\n\n\n983191049\n\n\n-61.38301\n\n\n15.29255\n\n\n2015-02-20 01:11:35\n\n\nPOINT (465272.964 1690049.908)\n\n\n0\n\n\n167411 rows × 7 columns\n\n\nShift each observation down one row in our copied ship dataframe:\nhabitat_ais_copy = habitat_ais_copy.shift(periods = 1)\nhabitat_ais_copy\n\n\n\n\nfield_1\n\n\nMMSI\n\n\nLON\n\n\nLAT\n\n\nTIMESTAMP\n\n\ngeometry\n\n\nindex_right\n\n\n235025\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaT\n\n\nNone\n\n\nNaN\n\n\n235018\n\n\n235025\n\n\n203106200\n\n\n-61.40929\n\n\n15.21021\n\n\n2015-02-25 15:32:20\n\n\nPOINT (462476.396 1680935.224)\n\n\n0.0\n\n\n235000\n\n\n235018\n\n\n203106200\n\n\n-61.41107\n\n\n15.21436\n\n\n2015-02-25 15:34:50\n\n\nPOINT (462283.995 1681393.698)\n\n\n0.0\n\n\n234989\n\n\n235000\n\n\n203106200\n\n\n-61.41427\n\n\n15.22638\n\n\n2015-02-25 15:42:19\n\n\nPOINT (461936.769 1682722.187)\n\n\n0.0\n\n\n234984\n\n\n234989\n\n\n203106200\n\n\n-61.41553\n\n\n15.2353\n\n\n2015-02-25 15:47:19\n\n\nPOINT (461798.818 1683708.377)\n\n\n0.0\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n259103\n\n\n259118\n\n\n983191049\n\n\n-61.38323\n\n\n15.29282\n\n\n2015-02-19 19:44:46\n\n\nPOINT (465249.261 1690079.703)\n\n\n0.0\n\n\n259094\n\n\n259103\n\n\n983191049\n\n\n-61.38322\n\n\n15.2927\n\n\n2015-02-19 19:50:45\n\n\nPOINT (465250.372 1690066.434)\n\n\n0.0\n\n\n258954\n\n\n259094\n\n\n983191049\n\n\n-61.38328\n\n\n15.29259\n\n\n2015-02-19 19:55:09\n\n\nPOINT (465243.965 1690054.249)\n\n\n0.0\n\n\n258930\n\n\n258954\n\n\n983191049\n\n\n-61.38344\n\n\n15.2932\n\n\n2015-02-19 20:51:12\n\n\nPOINT (465226.597 1690121.667)\n\n\n0.0\n\n\n258206\n\n\n258930\n\n\n983191049\n\n\n-61.38329\n\n\n15.29258\n\n\n2015-02-19 21:02:54\n\n\nPOINT (465242.895 1690053.140)\n\n\n0.0\n\n\n167411 rows × 7 columns\n\n\nJoin original dataframe with shifted copy using join():\nais_joined = habitat_ais_copy.join(habitat_ais_gdf, how = 'left', lsuffix = '_2', sort = False)\nais_joined\n\n\n\n\nfield_1_2\n\n\nMMSI_2\n\n\nLON_2\n\n\nLAT_2\n\n\nTIMESTAMP_2\n\n\ngeometry_2\n\n\nindex_right_2\n\n\nfield_1\n\n\nMMSI\n\n\nLON\n\n\nLAT\n\n\nTIMESTAMP\n\n\ngeometry\n\n\nindex_right\n\n\n235025\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaT\n\n\nNone\n\n\nNaN\n\n\n235025\n\n\n203106200\n\n\n-61.40929\n\n\n15.21021\n\n\n2015-02-25 15:32:20\n\n\nPOINT (462476.396 1680935.224)\n\n\n0\n\n\n235018\n\n\n235025\n\n\n203106200\n\n\n-61.40929\n\n\n15.21021\n\n\n2015-02-25 15:32:20\n\n\nPOINT (462476.396 1680935.224)\n\n\n0.0\n\n\n235018\n\n\n203106200\n\n\n-61.41107\n\n\n15.21436\n\n\n2015-02-25 15:34:50\n\n\nPOINT (462283.995 1681393.698)\n\n\n0\n\n\n235000\n\n\n235018\n\n\n203106200\n\n\n-61.41107\n\n\n15.21436\n\n\n2015-02-25 15:34:50\n\n\nPOINT (462283.995 1681393.698)\n\n\n0.0\n\n\n235000\n\n\n203106200\n\n\n-61.41427\n\n\n15.22638\n\n\n2015-02-25 15:42:19\n\n\nPOINT (461936.769 1682722.187)\n\n\n0\n\n\n234989\n\n\n235000\n\n\n203106200\n\n\n-61.41427\n\n\n15.22638\n\n\n2015-02-25 15:42:19\n\n\nPOINT (461936.769 1682722.187)\n\n\n0.0\n\n\n234989\n\n\n203106200\n\n\n-61.41553\n\n\n15.2353\n\n\n2015-02-25 15:47:19\n\n\nPOINT (461798.818 1683708.377)\n\n\n0\n\n\n234984\n\n\n234989\n\n\n203106200\n\n\n-61.41553\n\n\n15.2353\n\n\n2015-02-25 15:47:19\n\n\nPOINT (461798.818 1683708.377)\n\n\n0.0\n\n\n234984\n\n\n203106200\n\n\n-61.41687\n\n\n15.23792\n\n\n2015-02-25 15:49:50\n\n\nPOINT (461654.150 1683997.765)\n\n\n0\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n259103\n\n\n259118\n\n\n983191049\n\n\n-61.38323\n\n\n15.29282\n\n\n2015-02-19 19:44:46\n\n\nPOINT (465249.261 1690079.703)\n\n\n0.0\n\n\n259103\n\n\n983191049\n\n\n-61.38322\n\n\n15.2927\n\n\n2015-02-19 19:50:45\n\n\nPOINT (465250.372 1690066.434)\n\n\n0\n\n\n259094\n\n\n259103\n\n\n983191049\n\n\n-61.38322\n\n\n15.2927\n\n\n2015-02-19 19:50:45\n\n\nPOINT (465250.372 1690066.434)\n\n\n0.0\n\n\n259094\n\n\n983191049\n\n\n-61.38328\n\n\n15.29259\n\n\n2015-02-19 19:55:09\n\n\nPOINT (465243.965 1690054.249)\n\n\n0\n\n\n258954\n\n\n259094\n\n\n983191049\n\n\n-61.38328\n\n\n15.29259\n\n\n2015-02-19 19:55:09\n\n\nPOINT (465243.965 1690054.249)\n\n\n0.0\n\n\n258954\n\n\n983191049\n\n\n-61.38344\n\n\n15.2932\n\n\n2015-02-19 20:51:12\n\n\nPOINT (465226.597 1690121.667)\n\n\n0\n\n\n258930\n\n\n258954\n\n\n983191049\n\n\n-61.38344\n\n\n15.2932\n\n\n2015-02-19 20:51:12\n\n\nPOINT (465226.597 1690121.667)\n\n\n0.0\n\n\n258930\n\n\n983191049\n\n\n-61.38329\n\n\n15.29258\n\n\n2015-02-19 21:02:54\n\n\nPOINT (465242.895 1690053.140)\n\n\n0\n\n\n258206\n\n\n258930\n\n\n983191049\n\n\n-61.38329\n\n\n15.29258\n\n\n2015-02-19 21:02:54\n\n\nPOINT (465242.895 1690053.140)\n\n\n0.0\n\n\n258206\n\n\n983191049\n\n\n-61.38301\n\n\n15.29255\n\n\n2015-02-20 01:11:35\n\n\nPOINT (465272.964 1690049.908)\n\n\n0\n\n\n167411 rows × 14 columns\n\n\nNext, we are looking to examine distances traveled by ships using their AIS data per row, so we eliminate rows in which MMSI for ship 1 and 2 are not the same to align the data:\nais_joined = ais_joined[ais_joined['MMSI_2'] == ais_joined['MMSI']]\nais_joined\n\n\n\n\nfield_1_2\n\n\nMMSI_2\n\n\nLON_2\n\n\nLAT_2\n\n\nTIMESTAMP_2\n\n\ngeometry_2\n\n\nindex_right_2\n\n\nfield_1\n\n\nMMSI\n\n\nLON\n\n\nLAT\n\n\nTIMESTAMP\n\n\ngeometry\n\n\nindex_right\n\n\n235018\n\n\n235025\n\n\n203106200\n\n\n-61.40929\n\n\n15.21021\n\n\n2015-02-25 15:32:20\n\n\nPOINT (462476.396 1680935.224)\n\n\n0.0\n\n\n235018\n\n\n203106200\n\n\n-61.41107\n\n\n15.21436\n\n\n2015-02-25 15:34:50\n\n\nPOINT (462283.995 1681393.698)\n\n\n0\n\n\n235000\n\n\n235018\n\n\n203106200\n\n\n-61.41107\n\n\n15.21436\n\n\n2015-02-25 15:34:50\n\n\nPOINT (462283.995 1681393.698)\n\n\n0.0\n\n\n235000\n\n\n203106200\n\n\n-61.41427\n\n\n15.22638\n\n\n2015-02-25 15:42:19\n\n\nPOINT (461936.769 1682722.187)\n\n\n0\n\n\n234989\n\n\n235000\n\n\n203106200\n\n\n-61.41427\n\n\n15.22638\n\n\n2015-02-25 15:42:19\n\n\nPOINT (461936.769 1682722.187)\n\n\n0.0\n\n\n234989\n\n\n203106200\n\n\n-61.41553\n\n\n15.2353\n\n\n2015-02-25 15:47:19\n\n\nPOINT (461798.818 1683708.377)\n\n\n0\n\n\n234984\n\n\n234989\n\n\n203106200\n\n\n-61.41553\n\n\n15.2353\n\n\n2015-02-25 15:47:19\n\n\nPOINT (461798.818 1683708.377)\n\n\n0.0\n\n\n234984\n\n\n203106200\n\n\n-61.41687\n\n\n15.23792\n\n\n2015-02-25 15:49:50\n\n\nPOINT (461654.150 1683997.765)\n\n\n0\n\n\n234972\n\n\n234984\n\n\n203106200\n\n\n-61.41687\n\n\n15.23792\n\n\n2015-02-25 15:49:50\n\n\nPOINT (461654.150 1683997.765)\n\n\n0.0\n\n\n234972\n\n\n203106200\n\n\n-61.41851\n\n\n15.24147\n\n\n2015-02-25 15:54:49\n\n\nPOINT (461476.997 1684389.925)\n\n\n0\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n259103\n\n\n259118\n\n\n983191049\n\n\n-61.38323\n\n\n15.29282\n\n\n2015-02-19 19:44:46\n\n\nPOINT (465249.261 1690079.703)\n\n\n0.0\n\n\n259103\n\n\n983191049\n\n\n-61.38322\n\n\n15.2927\n\n\n2015-02-19 19:50:45\n\n\nPOINT (465250.372 1690066.434)\n\n\n0\n\n\n259094\n\n\n259103\n\n\n983191049\n\n\n-61.38322\n\n\n15.2927\n\n\n2015-02-19 19:50:45\n\n\nPOINT (465250.372 1690066.434)\n\n\n0.0\n\n\n259094\n\n\n983191049\n\n\n-61.38328\n\n\n15.29259\n\n\n2015-02-19 19:55:09\n\n\nPOINT (465243.965 1690054.249)\n\n\n0\n\n\n258954\n\n\n259094\n\n\n983191049\n\n\n-61.38328\n\n\n15.29259\n\n\n2015-02-19 19:55:09\n\n\nPOINT (465243.965 1690054.249)\n\n\n0.0\n\n\n258954\n\n\n983191049\n\n\n-61.38344\n\n\n15.2932\n\n\n2015-02-19 20:51:12\n\n\nPOINT (465226.597 1690121.667)\n\n\n0\n\n\n258930\n\n\n258954\n\n\n983191049\n\n\n-61.38344\n\n\n15.2932\n\n\n2015-02-19 20:51:12\n\n\nPOINT (465226.597 1690121.667)\n\n\n0.0\n\n\n258930\n\n\n983191049\n\n\n-61.38329\n\n\n15.29258\n\n\n2015-02-19 21:02:54\n\n\nPOINT (465242.895 1690053.140)\n\n\n0\n\n\n258206\n\n\n258930\n\n\n983191049\n\n\n-61.38329\n\n\n15.29258\n\n\n2015-02-19 21:02:54\n\n\nPOINT (465242.895 1690053.140)\n\n\n0.0\n\n\n258206\n\n\n983191049\n\n\n-61.38301\n\n\n15.29255\n\n\n2015-02-20 01:11:35\n\n\nPOINT (465272.964 1690049.908)\n\n\n0\n\n\n166255 rows × 14 columns\n\n\nSet the geometry for our joined data frame:\nais_joined = ais_joined.set_geometry(ais_joined['geometry'])\nReproject to Dominica 1945 as our CRS:\nais_joined = ais_joined.to_crs(\"EPSG:2002\")\nAdd a column in which we calculate the distance traveled between the first and second geometry:\nais_joined['distance'] = ais_joined['geometry'].distance(ais_joined['geometry_2'])\nais_joined\n\n\n\n\nfield_1_2\n\n\nMMSI_2\n\n\nLON_2\n\n\nLAT_2\n\n\nTIMESTAMP_2\n\n\ngeometry_2\n\n\nindex_right_2\n\n\nfield_1\n\n\nMMSI\n\n\nLON\n\n\nLAT\n\n\nTIMESTAMP\n\n\ngeometry\n\n\nindex_right\n\n\ndistance\n\n\n235018\n\n\n235025\n\n\n203106200\n\n\n-61.40929\n\n\n15.21021\n\n\n2015-02-25 15:32:20\n\n\nPOINT (462476.396 1680935.224)\n\n\n0.0\n\n\n235018\n\n\n203106200\n\n\n-61.41107\n\n\n15.21436\n\n\n2015-02-25 15:34:50\n\n\nPOINT (462283.995 1681393.698)\n\n\n0\n\n\n497.209041\n\n\n235000\n\n\n235018\n\n\n203106200\n\n\n-61.41107\n\n\n15.21436\n\n\n2015-02-25 15:34:50\n\n\nPOINT (462283.995 1681393.698)\n\n\n0.0\n\n\n235000\n\n\n203106200\n\n\n-61.41427\n\n\n15.22638\n\n\n2015-02-25 15:42:19\n\n\nPOINT (461936.769 1682722.187)\n\n\n0\n\n\n1373.116137\n\n\n234989\n\n\n235000\n\n\n203106200\n\n\n-61.41427\n\n\n15.22638\n\n\n2015-02-25 15:42:19\n\n\nPOINT (461936.769 1682722.187)\n\n\n0.0\n\n\n234989\n\n\n203106200\n\n\n-61.41553\n\n\n15.2353\n\n\n2015-02-25 15:47:19\n\n\nPOINT (461798.818 1683708.377)\n\n\n0\n\n\n995.792381\n\n\n234984\n\n\n234989\n\n\n203106200\n\n\n-61.41553\n\n\n15.2353\n\n\n2015-02-25 15:47:19\n\n\nPOINT (461798.818 1683708.377)\n\n\n0.0\n\n\n234984\n\n\n203106200\n\n\n-61.41687\n\n\n15.23792\n\n\n2015-02-25 15:49:50\n\n\nPOINT (461654.150 1683997.765)\n\n\n0\n\n\n323.533223\n\n\n234972\n\n\n234984\n\n\n203106200\n\n\n-61.41687\n\n\n15.23792\n\n\n2015-02-25 15:49:50\n\n\nPOINT (461654.150 1683997.765)\n\n\n0.0\n\n\n234972\n\n\n203106200\n\n\n-61.41851\n\n\n15.24147\n\n\n2015-02-25 15:54:49\n\n\nPOINT (461476.997 1684389.925)\n\n\n0\n\n\n430.317090\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n259103\n\n\n259118\n\n\n983191049\n\n\n-61.38323\n\n\n15.29282\n\n\n2015-02-19 19:44:46\n\n\nPOINT (465249.261 1690079.703)\n\n\n0.0\n\n\n259103\n\n\n983191049\n\n\n-61.38322\n\n\n15.2927\n\n\n2015-02-19 19:50:45\n\n\nPOINT (465250.372 1690066.434)\n\n\n0\n\n\n13.315561\n\n\n259094\n\n\n259103\n\n\n983191049\n\n\n-61.38322\n\n\n15.2927\n\n\n2015-02-19 19:50:45\n\n\nPOINT (465250.372 1690066.434)\n\n\n0.0\n\n\n259094\n\n\n983191049\n\n\n-61.38328\n\n\n15.29259\n\n\n2015-02-19 19:55:09\n\n\nPOINT (465243.965 1690054.249)\n\n\n0\n\n\n13.766128\n\n\n258954\n\n\n259094\n\n\n983191049\n\n\n-61.38328\n\n\n15.29259\n\n\n2015-02-19 19:55:09\n\n\nPOINT (465243.965 1690054.249)\n\n\n0.0\n\n\n258954\n\n\n983191049\n\n\n-61.38344\n\n\n15.2932\n\n\n2015-02-19 20:51:12\n\n\nPOINT (465226.597 1690121.667)\n\n\n0\n\n\n69.619301\n\n\n258930\n\n\n258954\n\n\n983191049\n\n\n-61.38344\n\n\n15.2932\n\n\n2015-02-19 20:51:12\n\n\nPOINT (465226.597 1690121.667)\n\n\n0.0\n\n\n258930\n\n\n983191049\n\n\n-61.38329\n\n\n15.29258\n\n\n2015-02-19 21:02:54\n\n\nPOINT (465242.895 1690053.140)\n\n\n0\n\n\n70.438502\n\n\n258206\n\n\n258930\n\n\n983191049\n\n\n-61.38329\n\n\n15.29258\n\n\n2015-02-19 21:02:54\n\n\nPOINT (465242.895 1690053.140)\n\n\n0.0\n\n\n258206\n\n\n983191049\n\n\n-61.38301\n\n\n15.29255\n\n\n2015-02-20 01:11:35\n\n\nPOINT (465272.964 1690049.908)\n\n\n0\n\n\n30.241849\n\n\n166255 rows × 15 columns\n\n\nAdd a column in which we calculate the time passed as ships traveled between the first and second geometry:\nais_joined['time_passed'] = abs(ais_joined['TIMESTAMP'] - ais_joined['TIMESTAMP_2'])\nais_joined\n\n\n\n\nfield_1_2\n\n\nMMSI_2\n\n\nLON_2\n\n\nLAT_2\n\n\nTIMESTAMP_2\n\n\ngeometry_2\n\n\nindex_right_2\n\n\nfield_1\n\n\nMMSI\n\n\nLON\n\n\nLAT\n\n\nTIMESTAMP\n\n\ngeometry\n\n\nindex_right\n\n\ndistance\n\n\ntime_passed\n\n\n235018\n\n\n235025\n\n\n203106200\n\n\n-61.40929\n\n\n15.21021\n\n\n2015-02-25 15:32:20\n\n\nPOINT (462476.396 1680935.224)\n\n\n0.0\n\n\n235018\n\n\n203106200\n\n\n-61.41107\n\n\n15.21436\n\n\n2015-02-25 15:34:50\n\n\nPOINT (462283.995 1681393.698)\n\n\n0\n\n\n497.209041\n\n\n0 days 00:02:30\n\n\n235000\n\n\n235018\n\n\n203106200\n\n\n-61.41107\n\n\n15.21436\n\n\n2015-02-25 15:34:50\n\n\nPOINT (462283.995 1681393.698)\n\n\n0.0\n\n\n235000\n\n\n203106200\n\n\n-61.41427\n\n\n15.22638\n\n\n2015-02-25 15:42:19\n\n\nPOINT (461936.769 1682722.187)\n\n\n0\n\n\n1373.116137\n\n\n0 days 00:07:29\n\n\n234989\n\n\n235000\n\n\n203106200\n\n\n-61.41427\n\n\n15.22638\n\n\n2015-02-25 15:42:19\n\n\nPOINT (461936.769 1682722.187)\n\n\n0.0\n\n\n234989\n\n\n203106200\n\n\n-61.41553\n\n\n15.2353\n\n\n2015-02-25 15:47:19\n\n\nPOINT (461798.818 1683708.377)\n\n\n0\n\n\n995.792381\n\n\n0 days 00:05:00\n\n\n234984\n\n\n234989\n\n\n203106200\n\n\n-61.41553\n\n\n15.2353\n\n\n2015-02-25 15:47:19\n\n\nPOINT (461798.818 1683708.377)\n\n\n0.0\n\n\n234984\n\n\n203106200\n\n\n-61.41687\n\n\n15.23792\n\n\n2015-02-25 15:49:50\n\n\nPOINT (461654.150 1683997.765)\n\n\n0\n\n\n323.533223\n\n\n0 days 00:02:31\n\n\n234972\n\n\n234984\n\n\n203106200\n\n\n-61.41687\n\n\n15.23792\n\n\n2015-02-25 15:49:50\n\n\nPOINT (461654.150 1683997.765)\n\n\n0.0\n\n\n234972\n\n\n203106200\n\n\n-61.41851\n\n\n15.24147\n\n\n2015-02-25 15:54:49\n\n\nPOINT (461476.997 1684389.925)\n\n\n0\n\n\n430.317090\n\n\n0 days 00:04:59\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n259103\n\n\n259118\n\n\n983191049\n\n\n-61.38323\n\n\n15.29282\n\n\n2015-02-19 19:44:46\n\n\nPOINT (465249.261 1690079.703)\n\n\n0.0\n\n\n259103\n\n\n983191049\n\n\n-61.38322\n\n\n15.2927\n\n\n2015-02-19 19:50:45\n\n\nPOINT (465250.372 1690066.434)\n\n\n0\n\n\n13.315561\n\n\n0 days 00:05:59\n\n\n259094\n\n\n259103\n\n\n983191049\n\n\n-61.38322\n\n\n15.2927\n\n\n2015-02-19 19:50:45\n\n\nPOINT (465250.372 1690066.434)\n\n\n0.0\n\n\n259094\n\n\n983191049\n\n\n-61.38328\n\n\n15.29259\n\n\n2015-02-19 19:55:09\n\n\nPOINT (465243.965 1690054.249)\n\n\n0\n\n\n13.766128\n\n\n0 days 00:04:24\n\n\n258954\n\n\n259094\n\n\n983191049\n\n\n-61.38328\n\n\n15.29259\n\n\n2015-02-19 19:55:09\n\n\nPOINT (465243.965 1690054.249)\n\n\n0.0\n\n\n258954\n\n\n983191049\n\n\n-61.38344\n\n\n15.2932\n\n\n2015-02-19 20:51:12\n\n\nPOINT (465226.597 1690121.667)\n\n\n0\n\n\n69.619301\n\n\n0 days 00:56:03\n\n\n258930\n\n\n258954\n\n\n983191049\n\n\n-61.38344\n\n\n15.2932\n\n\n2015-02-19 20:51:12\n\n\nPOINT (465226.597 1690121.667)\n\n\n0.0\n\n\n258930\n\n\n983191049\n\n\n-61.38329\n\n\n15.29258\n\n\n2015-02-19 21:02:54\n\n\nPOINT (465242.895 1690053.140)\n\n\n0\n\n\n70.438502\n\n\n0 days 00:11:42\n\n\n258206\n\n\n258930\n\n\n983191049\n\n\n-61.38329\n\n\n15.29258\n\n\n2015-02-19 21:02:54\n\n\nPOINT (465242.895 1690053.140)\n\n\n0.0\n\n\n258206\n\n\n983191049\n\n\n-61.38301\n\n\n15.29255\n\n\n2015-02-20 01:11:35\n\n\nPOINT (465272.964 1690049.908)\n\n\n0\n\n\n30.241849\n\n\n0 days 04:08:41\n\n\n166255 rows × 16 columns\n\n\nNext, we convert the time passed to seconds for the purpose of calculating speed using the commands time_delta and total_seconds:\nais_joined['time_seconds'] = pd.to_timedelta(ais_joined.time_passed, errors='coerce').dt.total_seconds()\nais_joined\n\n\n\n\nfield_1_2\n\n\nMMSI_2\n\n\nLON_2\n\n\nLAT_2\n\n\nTIMESTAMP_2\n\n\ngeometry_2\n\n\nindex_right_2\n\n\nfield_1\n\n\nMMSI\n\n\nLON\n\n\nLAT\n\n\nTIMESTAMP\n\n\ngeometry\n\n\nindex_right\n\n\ndistance\n\n\ntime_passed\n\n\ntime_seconds\n\n\n235018\n\n\n235025\n\n\n203106200\n\n\n-61.40929\n\n\n15.21021\n\n\n2015-02-25 15:32:20\n\n\nPOINT (462476.396 1680935.224)\n\n\n0.0\n\n\n235018\n\n\n203106200\n\n\n-61.41107\n\n\n15.21436\n\n\n2015-02-25 15:34:50\n\n\nPOINT (462283.995 1681393.698)\n\n\n0\n\n\n497.209041\n\n\n0 days 00:02:30\n\n\n150.0\n\n\n235000\n\n\n235018\n\n\n203106200\n\n\n-61.41107\n\n\n15.21436\n\n\n2015-02-25 15:34:50\n\n\nPOINT (462283.995 1681393.698)\n\n\n0.0\n\n\n235000\n\n\n203106200\n\n\n-61.41427\n\n\n15.22638\n\n\n2015-02-25 15:42:19\n\n\nPOINT (461936.769 1682722.187)\n\n\n0\n\n\n1373.116137\n\n\n0 days 00:07:29\n\n\n449.0\n\n\n234989\n\n\n235000\n\n\n203106200\n\n\n-61.41427\n\n\n15.22638\n\n\n2015-02-25 15:42:19\n\n\nPOINT (461936.769 1682722.187)\n\n\n0.0\n\n\n234989\n\n\n203106200\n\n\n-61.41553\n\n\n15.2353\n\n\n2015-02-25 15:47:19\n\n\nPOINT (461798.818 1683708.377)\n\n\n0\n\n\n995.792381\n\n\n0 days 00:05:00\n\n\n300.0\n\n\n234984\n\n\n234989\n\n\n203106200\n\n\n-61.41553\n\n\n15.2353\n\n\n2015-02-25 15:47:19\n\n\nPOINT (461798.818 1683708.377)\n\n\n0.0\n\n\n234984\n\n\n203106200\n\n\n-61.41687\n\n\n15.23792\n\n\n2015-02-25 15:49:50\n\n\nPOINT (461654.150 1683997.765)\n\n\n0\n\n\n323.533223\n\n\n0 days 00:02:31\n\n\n151.0\n\n\n234972\n\n\n234984\n\n\n203106200\n\n\n-61.41687\n\n\n15.23792\n\n\n2015-02-25 15:49:50\n\n\nPOINT (461654.150 1683997.765)\n\n\n0.0\n\n\n234972\n\n\n203106200\n\n\n-61.41851\n\n\n15.24147\n\n\n2015-02-25 15:54:49\n\n\nPOINT (461476.997 1684389.925)\n\n\n0\n\n\n430.317090\n\n\n0 days 00:04:59\n\n\n299.0\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n259103\n\n\n259118\n\n\n983191049\n\n\n-61.38323\n\n\n15.29282\n\n\n2015-02-19 19:44:46\n\n\nPOINT (465249.261 1690079.703)\n\n\n0.0\n\n\n259103\n\n\n983191049\n\n\n-61.38322\n\n\n15.2927\n\n\n2015-02-19 19:50:45\n\n\nPOINT (465250.372 1690066.434)\n\n\n0\n\n\n13.315561\n\n\n0 days 00:05:59\n\n\n359.0\n\n\n259094\n\n\n259103\n\n\n983191049\n\n\n-61.38322\n\n\n15.2927\n\n\n2015-02-19 19:50:45\n\n\nPOINT (465250.372 1690066.434)\n\n\n0.0\n\n\n259094\n\n\n983191049\n\n\n-61.38328\n\n\n15.29259\n\n\n2015-02-19 19:55:09\n\n\nPOINT (465243.965 1690054.249)\n\n\n0\n\n\n13.766128\n\n\n0 days 00:04:24\n\n\n264.0\n\n\n258954\n\n\n259094\n\n\n983191049\n\n\n-61.38328\n\n\n15.29259\n\n\n2015-02-19 19:55:09\n\n\nPOINT (465243.965 1690054.249)\n\n\n0.0\n\n\n258954\n\n\n983191049\n\n\n-61.38344\n\n\n15.2932\n\n\n2015-02-19 20:51:12\n\n\nPOINT (465226.597 1690121.667)\n\n\n0\n\n\n69.619301\n\n\n0 days 00:56:03\n\n\n3363.0\n\n\n258930\n\n\n258954\n\n\n983191049\n\n\n-61.38344\n\n\n15.2932\n\n\n2015-02-19 20:51:12\n\n\nPOINT (465226.597 1690121.667)\n\n\n0.0\n\n\n258930\n\n\n983191049\n\n\n-61.38329\n\n\n15.29258\n\n\n2015-02-19 21:02:54\n\n\nPOINT (465242.895 1690053.140)\n\n\n0\n\n\n70.438502\n\n\n0 days 00:11:42\n\n\n702.0\n\n\n258206\n\n\n258930\n\n\n983191049\n\n\n-61.38329\n\n\n15.29258\n\n\n2015-02-19 21:02:54\n\n\nPOINT (465242.895 1690053.140)\n\n\n0.0\n\n\n258206\n\n\n983191049\n\n\n-61.38301\n\n\n15.29255\n\n\n2015-02-20 01:11:35\n\n\nPOINT (465272.964 1690049.908)\n\n\n0\n\n\n30.241849\n\n\n0 days 04:08:41\n\n\n14921.0\n\n\n166255 rows × 17 columns\n\n\nThen, we add a column for speed, which is calculated by dividing the distance travelled by the time in seconds:\nais_joined_2['avg_speed_mps'] = ais_joined_2['distance'] / ais_joined_2['time_seconds'] \nais_joined_2\n\n\n\n\nfield_1_2\n\n\nMMSI_2\n\n\nLON_2\n\n\nLAT_2\n\n\nTIMESTAMP_2\n\n\ngeometry_2\n\n\nfield_1\n\n\nMMSI\n\n\nLON\n\n\nLAT\n\n\nTIMESTAMP\n\n\ngeometry\n\n\ndistance\n\n\ntime_passed\n\n\ntime_seconds\n\n\navg_speed\n\n\ntime_for_10_knots\n\n\ntime_difference\n\n\navg_speed_mps\n\n\n511094\n\n\n132562\n\n\n1184\n\n\n-61.30541\n\n\n14.79982\n\n\n2015-07-21 12:27:51\n\n\nPOINT (473775.169 1635578.904)\n\n\n132559\n\n\n1184\n\n\n-61.39245\n\n\n14.90217\n\n\n2015-07-21 12:30:31\n\n\nPOINT (464378.762 1646871.313)\n\n\n14690.505921\n\n\n0 days 00:02:40\n\n\n160.0\n\n\n91.815662\n\n\n285.563057\n\n\n125.563057\n\n\n91.815662\n\n\n511090\n\n\n132559\n\n\n1184\n\n\n-61.39245\n\n\n14.90217\n\n\n2015-07-21 12:30:31\n\n\nPOINT (464378.762 1646871.313)\n\n\n132555\n\n\n1184\n\n\n-61.48131\n\n\n15.00648\n\n\n2015-07-21 12:33:12\n\n\nPOINT (454794.970 1658383.473)\n\n\n14979.281904\n\n\n0 days 00:02:41\n\n\n161.0\n\n\n93.039018\n\n\n291.176462\n\n\n130.176462\n\n\n93.039018\n\n\n511087\n\n\n132555\n\n\n1184\n\n\n-61.48131\n\n\n15.00648\n\n\n2015-07-21 12:33:12\n\n\nPOINT (454794.970 1658383.473)\n\n\n132552\n\n\n1184\n\n\n-61.54865\n\n\n15.08558\n\n\n2015-07-21 12:35:12\n\n\nPOINT (447538.279 1667115.779)\n\n\n11353.974711\n\n\n0 days 00:02:00\n\n\n120.0\n\n\n94.616456\n\n\n220.705519\n\n\n100.705519\n\n\n94.616456\n\n\n511084\n\n\n132552\n\n\n1184\n\n\n-61.54865\n\n\n15.08558\n\n\n2015-07-21 12:35:12\n\n\nPOINT (447538.279 1667115.779)\n\n\n132549\n\n\n1184\n\n\n-61.61598\n\n\n15.1644\n\n\n2015-07-21 12:37:12\n\n\nPOINT (440288.034 1675819.254)\n\n\n11327.688520\n\n\n0 days 00:02:00\n\n\n120.0\n\n\n94.397404\n\n\n220.194552\n\n\n100.194552\n\n\n94.397404\n\n\n511082\n\n\n132549\n\n\n1184\n\n\n-61.61598\n\n\n15.1644\n\n\n2015-07-21 12:37:12\n\n\nPOINT (440288.034 1675819.254)\n\n\n132547\n\n\n1184\n\n\n-61.6842\n\n\n15.2442\n\n\n2015-07-21 12:39:12\n\n\nPOINT (432947.398 1684633.140)\n\n\n11470.375572\n\n\n0 days 00:02:00\n\n\n120.0\n\n\n95.586463\n\n\n222.968190\n\n\n102.968190\n\n\n95.586463\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n258930\n\n\n258954\n\n\n983191049\n\n\n-61.38344\n\n\n15.2932\n\n\n2015-02-19 20:51:12\n\n\nPOINT (465226.597 1690121.667)\n\n\n258930\n\n\n983191049\n\n\n-61.38329\n\n\n15.29258\n\n\n2015-02-19 21:02:54\n\n\nPOINT (465242.895 1690053.140)\n\n\n70.438502\n\n\n0 days 00:11:42\n\n\n702.0\n\n\n0.100340\n\n\n1.369227\n\n\n-700.630773\n\n\n0.100340\n\n\n258206\n\n\n258930\n\n\n983191049\n\n\n-61.38329\n\n\n15.29258\n\n\n2015-02-19 21:02:54\n\n\nPOINT (465242.895 1690053.140)\n\n\n258206\n\n\n983191049\n\n\n-61.38301\n\n\n15.29255\n\n\n2015-02-20 01:11:35\n\n\nPOINT (465272.964 1690049.908)\n\n\n30.241849\n\n\n0 days 04:08:41\n\n\n14921.0\n\n\n0.002027\n\n\n0.587860\n\n\n-14920.412140\n\n\n0.002027\n\n\n148161\n\n\n148164\n\n\n999000000\n\n\n-61.33341\n\n\n14.71754\n\n\n2015-03-23 09:50:22\n\n\nPOINT (470789.272 1626469.836)\n\n\n148161\n\n\n999000000\n\n\n-61.329\n\n\n14.71341\n\n\n2015-03-23 09:52:12\n\n\nPOINT (471265.334 1626014.478)\n\n\n658.775338\n\n\n0 days 00:01:50\n\n\n110.0\n\n\n5.988867\n\n\n12.805679\n\n\n-97.194321\n\n\n5.988867\n\n\n148155\n\n\n148161\n\n\n999000000\n\n\n-61.329\n\n\n14.71341\n\n\n2015-03-23 09:52:12\n\n\nPOINT (471265.334 1626014.478)\n\n\n148155\n\n\n999000000\n\n\n-61.32005\n\n\n14.70516\n\n\n2015-03-23 09:55:51\n\n\nPOINT (472231.502 1625104.938)\n\n\n1326.930261\n\n\n0 days 00:03:39\n\n\n219.0\n\n\n6.059042\n\n\n25.793684\n\n\n-193.206316\n\n\n6.059042\n\n\n148152\n\n\n148155\n\n\n999000000\n\n\n-61.32005\n\n\n14.70516\n\n\n2015-03-23 09:55:51\n\n\nPOINT (472231.502 1625104.938)\n\n\n148152\n\n\n999000000\n\n\n-61.31433\n\n\n14.6998\n\n\n2015-03-23 09:58:12\n\n\nPOINT (472849.053 1624514.001)\n\n\n854.737548\n\n\n0 days 00:02:21\n\n\n141.0\n\n\n6.061968\n\n\n16.614912\n\n\n-124.385088\n\n\n6.061968\n\n\n615312 rows × 19 columns\n\n\nTo calculate the time it would take each vessel to cover the distance they did if their speed were changed to 10 knots, we converted nautical miles to meters in order to find the time in seconds it would take each vessel to do this.\nm_per_nm = 1852\n\nais_joined['time_for_10_knots'] = (ais_joined['distance'] * 60 * 60) / (m_per_nm * 10)\nais_joined                \n\n\n\n\nfield_1_2\n\n\nMMSI_2\n\n\nLON_2\n\n\nLAT_2\n\n\nTIMESTAMP_2\n\n\ngeometry_2\n\n\nindex_right_2\n\n\nfield_1\n\n\nMMSI\n\n\nLON\n\n\nLAT\n\n\nTIMESTAMP\n\n\ngeometry\n\n\nindex_right\n\n\ndistance\n\n\ntime_passed\n\n\ntime_seconds\n\n\ntime_for_10_knots\n\n\ntime_difference\n\n\n235018\n\n\n235025\n\n\n203106200\n\n\n-61.40929\n\n\n15.21021\n\n\n2015-02-25 15:32:20\n\n\nPOINT (462476.396 1680935.224)\n\n\n0.0\n\n\n235018\n\n\n203106200\n\n\n-61.41107\n\n\n15.21436\n\n\n2015-02-25 15:34:50\n\n\nPOINT (462283.995 1681393.698)\n\n\n0\n\n\n497.209041\n\n\n0 days 00:02:30\n\n\n150.0\n\n\n96.649706\n\n\n-140.334946\n\n\n235000\n\n\n235018\n\n\n203106200\n\n\n-61.41107\n\n\n15.21436\n\n\n2015-02-25 15:34:50\n\n\nPOINT (462283.995 1681393.698)\n\n\n0.0\n\n\n235000\n\n\n203106200\n\n\n-61.41427\n\n\n15.22638\n\n\n2015-02-25 15:42:19\n\n\nPOINT (461936.769 1682722.187)\n\n\n0\n\n\n1373.116137\n\n\n0 days 00:07:29\n\n\n449.0\n\n\n266.912424\n\n\n-422.308527\n\n\n234989\n\n\n235000\n\n\n203106200\n\n\n-61.41427\n\n\n15.22638\n\n\n2015-02-25 15:42:19\n\n\nPOINT (461936.769 1682722.187)\n\n\n0.0\n\n\n234989\n\n\n203106200\n\n\n-61.41553\n\n\n15.2353\n\n\n2015-02-25 15:47:19\n\n\nPOINT (461798.818 1683708.377)\n\n\n0\n\n\n995.792381\n\n\n0 days 00:05:00\n\n\n300.0\n\n\n193.566553\n\n\n-280.643177\n\n\n234984\n\n\n234989\n\n\n203106200\n\n\n-61.41553\n\n\n15.2353\n\n\n2015-02-25 15:47:19\n\n\nPOINT (461798.818 1683708.377)\n\n\n0.0\n\n\n234984\n\n\n203106200\n\n\n-61.41687\n\n\n15.23792\n\n\n2015-02-25 15:49:50\n\n\nPOINT (461654.150 1683997.765)\n\n\n0\n\n\n323.533223\n\n\n0 days 00:02:31\n\n\n151.0\n\n\n62.889827\n\n\n-144.710963\n\n\n234972\n\n\n234984\n\n\n203106200\n\n\n-61.41687\n\n\n15.23792\n\n\n2015-02-25 15:49:50\n\n\nPOINT (461654.150 1683997.765)\n\n\n0.0\n\n\n234972\n\n\n203106200\n\n\n-61.41851\n\n\n15.24147\n\n\n2015-02-25 15:54:49\n\n\nPOINT (461476.997 1684389.925)\n\n\n0\n\n\n430.317090\n\n\n0 days 00:04:59\n\n\n299.0\n\n\n83.646951\n\n\n-290.635233\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n259103\n\n\n259118\n\n\n983191049\n\n\n-61.38323\n\n\n15.29282\n\n\n2015-02-19 19:44:46\n\n\nPOINT (465249.261 1690079.703)\n\n\n0.0\n\n\n259103\n\n\n983191049\n\n\n-61.38322\n\n\n15.2927\n\n\n2015-02-19 19:50:45\n\n\nPOINT (465250.372 1690066.434)\n\n\n0\n\n\n13.315561\n\n\n0 days 00:05:59\n\n\n359.0\n\n\n2.588338\n\n\n-358.741164\n\n\n259094\n\n\n259103\n\n\n983191049\n\n\n-61.38322\n\n\n15.2927\n\n\n2015-02-19 19:50:45\n\n\nPOINT (465250.372 1690066.434)\n\n\n0.0\n\n\n259094\n\n\n983191049\n\n\n-61.38328\n\n\n15.29259\n\n\n2015-02-19 19:55:09\n\n\nPOINT (465243.965 1690054.249)\n\n\n0\n\n\n13.766128\n\n\n0 days 00:04:24\n\n\n264.0\n\n\n2.675921\n\n\n-263.732406\n\n\n258954\n\n\n259094\n\n\n983191049\n\n\n-61.38328\n\n\n15.29259\n\n\n2015-02-19 19:55:09\n\n\nPOINT (465243.965 1690054.249)\n\n\n0.0\n\n\n258954\n\n\n983191049\n\n\n-61.38344\n\n\n15.2932\n\n\n2015-02-19 20:51:12\n\n\nPOINT (465226.597 1690121.667)\n\n\n0\n\n\n69.619301\n\n\n0 days 00:56:03\n\n\n3363.0\n\n\n13.532909\n\n\n-3361.646697\n\n\n258930\n\n\n258954\n\n\n983191049\n\n\n-61.38344\n\n\n15.2932\n\n\n2015-02-19 20:51:12\n\n\nPOINT (465226.597 1690121.667)\n\n\n0.0\n\n\n258930\n\n\n983191049\n\n\n-61.38329\n\n\n15.29258\n\n\n2015-02-19 21:02:54\n\n\nPOINT (465242.895 1690053.140)\n\n\n0\n\n\n70.438502\n\n\n0 days 00:11:42\n\n\n702.0\n\n\n13.692149\n\n\n-700.630773\n\n\n258206\n\n\n258930\n\n\n983191049\n\n\n-61.38329\n\n\n15.29258\n\n\n2015-02-19 21:02:54\n\n\nPOINT (465242.895 1690053.140)\n\n\n0.0\n\n\n258206\n\n\n983191049\n\n\n-61.38301\n\n\n15.29255\n\n\n2015-02-20 01:11:35\n\n\nPOINT (465272.964 1690049.908)\n\n\n0\n\n\n30.241849\n\n\n0 days 04:08:41\n\n\n14921.0\n\n\n5.878545\n\n\n-14920.412140\n\n\n166255 rows × 19 columns\n\n\nTo find the difference between the time that it actually took and how much it would have taken at 10 knots, we subtracted the time it actually took from the time it would have taken the vessels under 10 knots.\nais_joined['time_difference'] = ais_joined['time_for_10_knots'] - ais_joined['time_seconds']\nais_joined\n\n\n\n\nfield_1_2\n\n\nMMSI_2\n\n\nLON_2\n\n\nLAT_2\n\n\nTIMESTAMP_2\n\n\ngeometry_2\n\n\nindex_right_2\n\n\nfield_1\n\n\nMMSI\n\n\nLON\n\n\nLAT\n\n\nTIMESTAMP\n\n\ngeometry\n\n\nindex_right\n\n\ndistance\n\n\ntime_passed\n\n\ntime_seconds\n\n\ntime_for_10_knots\n\n\ntime_difference\n\n\n235018\n\n\n235025\n\n\n203106200\n\n\n-61.40929\n\n\n15.21021\n\n\n2015-02-25 15:32:20\n\n\nPOINT (462476.396 1680935.224)\n\n\n0.0\n\n\n235018\n\n\n203106200\n\n\n-61.41107\n\n\n15.21436\n\n\n2015-02-25 15:34:50\n\n\nPOINT (462283.995 1681393.698)\n\n\n0\n\n\n497.209041\n\n\n0 days 00:02:30\n\n\n150.0\n\n\n96.649706\n\n\n-53.350294\n\n\n235000\n\n\n235018\n\n\n203106200\n\n\n-61.41107\n\n\n15.21436\n\n\n2015-02-25 15:34:50\n\n\nPOINT (462283.995 1681393.698)\n\n\n0.0\n\n\n235000\n\n\n203106200\n\n\n-61.41427\n\n\n15.22638\n\n\n2015-02-25 15:42:19\n\n\nPOINT (461936.769 1682722.187)\n\n\n0\n\n\n1373.116137\n\n\n0 days 00:07:29\n\n\n449.0\n\n\n266.912424\n\n\n-182.087576\n\n\n234989\n\n\n235000\n\n\n203106200\n\n\n-61.41427\n\n\n15.22638\n\n\n2015-02-25 15:42:19\n\n\nPOINT (461936.769 1682722.187)\n\n\n0.0\n\n\n234989\n\n\n203106200\n\n\n-61.41553\n\n\n15.2353\n\n\n2015-02-25 15:47:19\n\n\nPOINT (461798.818 1683708.377)\n\n\n0\n\n\n995.792381\n\n\n0 days 00:05:00\n\n\n300.0\n\n\n193.566553\n\n\n-106.433447\n\n\n234984\n\n\n234989\n\n\n203106200\n\n\n-61.41553\n\n\n15.2353\n\n\n2015-02-25 15:47:19\n\n\nPOINT (461798.818 1683708.377)\n\n\n0.0\n\n\n234984\n\n\n203106200\n\n\n-61.41687\n\n\n15.23792\n\n\n2015-02-25 15:49:50\n\n\nPOINT (461654.150 1683997.765)\n\n\n0\n\n\n323.533223\n\n\n0 days 00:02:31\n\n\n151.0\n\n\n62.889827\n\n\n-88.110173\n\n\n234972\n\n\n234984\n\n\n203106200\n\n\n-61.41687\n\n\n15.23792\n\n\n2015-02-25 15:49:50\n\n\nPOINT (461654.150 1683997.765)\n\n\n0.0\n\n\n234972\n\n\n203106200\n\n\n-61.41851\n\n\n15.24147\n\n\n2015-02-25 15:54:49\n\n\nPOINT (461476.997 1684389.925)\n\n\n0\n\n\n430.317090\n\n\n0 days 00:04:59\n\n\n299.0\n\n\n83.646951\n\n\n-215.353049\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n259103\n\n\n259118\n\n\n983191049\n\n\n-61.38323\n\n\n15.29282\n\n\n2015-02-19 19:44:46\n\n\nPOINT (465249.261 1690079.703)\n\n\n0.0\n\n\n259103\n\n\n983191049\n\n\n-61.38322\n\n\n15.2927\n\n\n2015-02-19 19:50:45\n\n\nPOINT (465250.372 1690066.434)\n\n\n0\n\n\n13.315561\n\n\n0 days 00:05:59\n\n\n359.0\n\n\n2.588338\n\n\n-356.411662\n\n\n259094\n\n\n259103\n\n\n983191049\n\n\n-61.38322\n\n\n15.2927\n\n\n2015-02-19 19:50:45\n\n\nPOINT (465250.372 1690066.434)\n\n\n0.0\n\n\n259094\n\n\n983191049\n\n\n-61.38328\n\n\n15.29259\n\n\n2015-02-19 19:55:09\n\n\nPOINT (465243.965 1690054.249)\n\n\n0\n\n\n13.766128\n\n\n0 days 00:04:24\n\n\n264.0\n\n\n2.675921\n\n\n-261.324079\n\n\n258954\n\n\n259094\n\n\n983191049\n\n\n-61.38328\n\n\n15.29259\n\n\n2015-02-19 19:55:09\n\n\nPOINT (465243.965 1690054.249)\n\n\n0.0\n\n\n258954\n\n\n983191049\n\n\n-61.38344\n\n\n15.2932\n\n\n2015-02-19 20:51:12\n\n\nPOINT (465226.597 1690121.667)\n\n\n0\n\n\n69.619301\n\n\n0 days 00:56:03\n\n\n3363.0\n\n\n13.532909\n\n\n-3349.467091\n\n\n258930\n\n\n258954\n\n\n983191049\n\n\n-61.38344\n\n\n15.2932\n\n\n2015-02-19 20:51:12\n\n\nPOINT (465226.597 1690121.667)\n\n\n0.0\n\n\n258930\n\n\n983191049\n\n\n-61.38329\n\n\n15.29258\n\n\n2015-02-19 21:02:54\n\n\nPOINT (465242.895 1690053.140)\n\n\n0\n\n\n70.438502\n\n\n0 days 00:11:42\n\n\n702.0\n\n\n13.692149\n\n\n-688.307851\n\n\n258206\n\n\n258930\n\n\n983191049\n\n\n-61.38329\n\n\n15.29258\n\n\n2015-02-19 21:02:54\n\n\nPOINT (465242.895 1690053.140)\n\n\n0.0\n\n\n258206\n\n\n983191049\n\n\n-61.38301\n\n\n15.29255\n\n\n2015-02-20 01:11:35\n\n\nPOINT (465272.964 1690049.908)\n\n\n0\n\n\n30.241849\n\n\n0 days 04:08:41\n\n\n14921.0\n\n\n5.878545\n\n\n-14915.121455\n\n\n166255 rows × 19 columns\n\n\nNext, we took all measurements in the time_difference column > 0 to give us the vessels that actually travelled and therefor had positive time values.\nfast_ships = ais_joined[ais_joined['time_difference'] > 0]\nfast_ships\n\n\n\n\nfield_1_2\n\n\nMMSI_2\n\n\nLON_2\n\n\nLAT_2\n\n\nTIMESTAMP_2\n\n\ngeometry_2\n\n\nindex_right_2\n\n\nfield_1\n\n\nMMSI\n\n\nLON\n\n\nLAT\n\n\nTIMESTAMP\n\n\ngeometry\n\n\nindex_right\n\n\ndistance\n\n\ntime_passed\n\n\ntime_seconds\n\n\ntime_for_10_knots\n\n\ntime_difference\n\n\n53218\n\n\n53227\n\n\n203106200\n\n\n-61.42602\n\n\n15.35828\n\n\n2015-04-26 16:04:15\n\n\nPOINT (460636.434 1697307.052)\n\n\n0.0\n\n\n53218\n\n\n203106200\n\n\n-61.41927\n\n\n15.34898\n\n\n2015-04-26 16:08:16\n\n\nPOINT (461363.603 1696280.393)\n\n\n0\n\n\n1258.095562\n\n\n0 days 00:04:01\n\n\n241.0\n\n\n244.554213\n\n\n3.554213\n\n\n84957\n\n\n84977\n\n\n203518400\n\n\n-61.51328\n\n\n15.39394\n\n\n2015-04-14 17:28:32\n\n\nPOINT (451262.879 1701228.097)\n\n\n0.0\n\n\n84957\n\n\n203518400\n\n\n-61.51546\n\n\n15.41408\n\n\n2015-04-14 17:35:26\n\n\nPOINT (451023.962 1703455.061)\n\n\n0\n\n\n2239.743578\n\n\n0 days 00:06:54\n\n\n414.0\n\n\n435.371322\n\n\n21.371322\n\n\n39279\n\n\n39284\n\n\n205531510\n\n\n-61.40996\n\n\n15.34549\n\n\n2015-05-02 18:24:17\n\n\nPOINT (462363.849 1695897.095)\n\n\n0.0\n\n\n39279\n\n\n205531510\n\n\n-61.40981\n\n\n15.3516\n\n\n2015-05-02 18:26:15\n\n\nPOINT (462378.106 1696572.914)\n\n\n0\n\n\n675.968930\n\n\n0 days 00:01:58\n\n\n118.0\n\n\n131.397848\n\n\n13.397848\n\n\n139303\n\n\n139308\n\n\n205571000\n\n\n-61.51243\n\n\n15.33665\n\n\n2015-03-26 09:35:06\n\n\nPOINT (451368.368 1694892.030)\n\n\n0.0\n\n\n139303\n\n\n205571000\n\n\n-61.50684\n\n\n15.33412\n\n\n2015-03-26 09:37:06\n\n\nPOINT (451968.978 1694613.570)\n\n\n0\n\n\n662.020878\n\n\n0 days 00:02:00\n\n\n120.0\n\n\n128.686564\n\n\n8.686564\n\n\n139301\n\n\n139303\n\n\n205571000\n\n\n-61.50684\n\n\n15.33412\n\n\n2015-03-26 09:37:06\n\n\nPOINT (451968.978 1694613.570)\n\n\n0.0\n\n\n139301\n\n\n205571000\n\n\n-61.50115\n\n\n15.33156\n\n\n2015-03-26 09:39:06\n\n\nPOINT (452580.343 1694331.833)\n\n\n0\n\n\n673.159437\n\n\n0 days 00:02:00\n\n\n120.0\n\n\n130.851726\n\n\n10.851726\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n450065\n\n\n71532\n\n\n775907000\n\n\n-61.41703\n\n\n15.26152\n\n\n2015-08-31 15:22:58\n\n\nPOINT (461629.985 1686607.886)\n\n\n0.0\n\n\n71530\n\n\n775907000\n\n\n-61.41251\n\n\n15.26938\n\n\n2015-08-31 15:25:58\n\n\nPOINT (462112.948 1687478.511)\n\n\n0\n\n\n995.610920\n\n\n0 days 00:03:00\n\n\n180.0\n\n\n193.531280\n\n\n13.531280\n\n\n450061\n\n\n71530\n\n\n775907000\n\n\n-61.41251\n\n\n15.26938\n\n\n2015-08-31 15:25:58\n\n\nPOINT (462112.948 1687478.511)\n\n\n0.0\n\n\n71526\n\n\n775907000\n\n\n-61.40833\n\n\n15.27759\n\n\n2015-08-31 15:28:58\n\n\nPOINT (462559.266 1688387.759)\n\n\n0\n\n\n1012.882607\n\n\n0 days 00:03:00\n\n\n180.0\n\n\n196.888628\n\n\n16.888628\n\n\n450059\n\n\n71526\n\n\n775907000\n\n\n-61.40833\n\n\n15.27759\n\n\n2015-08-31 15:28:58\n\n\nPOINT (462559.266 1688387.759)\n\n\n0.0\n\n\n71524\n\n\n775907000\n\n\n-61.40408\n\n\n15.28582\n\n\n2015-08-31 15:31:59\n\n\nPOINT (463013.057 1689299.250)\n\n\n0\n\n\n1018.205189\n\n\n0 days 00:03:01\n\n\n181.0\n\n\n197.923255\n\n\n16.923255\n\n\n450056\n\n\n71524\n\n\n775907000\n\n\n-61.40408\n\n\n15.28582\n\n\n2015-08-31 15:31:59\n\n\nPOINT (463013.057 1689299.250)\n\n\n0.0\n\n\n71521\n\n\n775907000\n\n\n-61.39967\n\n\n15.29361\n\n\n2015-08-31 15:34:59\n\n\nPOINT (463484.122 1690162.135)\n\n\n0\n\n\n983.093306\n\n\n0 days 00:03:00\n\n\n180.0\n\n\n191.098051\n\n\n11.098051\n\n\n172907\n\n\n172921\n\n\n982358863\n\n\n-61.40942\n\n\n15.35087\n\n\n2015-03-14 16:04:52\n\n\nPOINT (462420.182 1696492.289)\n\n\n0.0\n\n\n172907\n\n\n982358863\n\n\n-61.40751\n\n\n15.34122\n\n\n2015-03-14 16:08:23\n\n\nPOINT (462628.091 1695425.545)\n\n\n0\n\n\n1086.815697\n\n\n0 days 00:03:31\n\n\n211.0\n\n\n211.260071\n\n\n0.260071\n\n\n21246 rows × 19 columns\n\n\nFinally, we summed up all of the time differences which represents our approximation to the impact on shipping (in terms of increased travel time) of a 10-knot reduced-speed zone in our identified whale habitat.\nseconds_lost = fast_ships['time_difference'].sum()\ndays_lost = seconds_lost / (60 * 60 * 24)\ndays_lost\n27.876040448213132\nWe can see from our calculations that a 10-knot reduced-speed zone would amount to a total of 27.89 day impact on shipping. While this number is may seem large, it is simply the total amount of time this reduction would cause across all vessels. In order to advocate for this regulation, it might be best to took at the additional time added to each invidudal vessel which we would expect be much lower overall.\n\n\n\n",
    "preview": {},
    "last_modified": "2022-03-30T11:16:34-07:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-01-17-avatar-the-last-airbender-who-drinks-the-most-tea/",
    "title": "Avatar the Last Airbender: Who Drinks the Most Tea?",
    "description": "Radial art in R using ggplot.",
    "author": [
      {
        "name": "Scout Leonard",
        "url": {
          "https://scoutcleonard.github.io": {}
        }
      }
    ],
    "date": "2022-01-17",
    "categories": [
      "tidy tuesday"
    ],
    "contents": "\nI do a bit of knitting, which feels comfortably creative because I can follow patterns and not make a single creative decision if I do not feel comfortable. It’s been an artistic outlet that has allowed me to lean into my creativity with some structure, which I like and need. This small project felt similar.\nCurious about aRt in R, I decided to watch the R Ladies Sana Barbara talk with Ijeamaka Anyene titled “aRt + ggplot: exploring radial visualizations”. I was pleased to find that the concepts were simple enough to try as I watched Ijeamaka share her gorgeous artwork. To me the simple, beautiful effect of adding + coord_polar to my ggplot bar graphs was amazing, and something I might not have considered on my own, much like new techniques in a knitting pattern :)\nLike Ijeamaka, I applied the techniques of radial visualizations to a Tidy Tuesday dataset from 2020 which contains the entire script for the Nickelodeon cartoon series, Avatar: the Last Airbender. I search for places where characters talk about tea, group by character to figure out who is doing th most talking about tea, and visualize “Who drinks the most tea?” I tweeted my results and received kind praise from Ijeamaka via her Twitter, which made me feel so COOL! I’m thankful for her generous instruction and aRtistry inspiration! She’s made me think harder about communicating data creatively, beautifully, and in unexpected ways!\nLoad Libraries\n\n\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(stringr)\nlibrary(ggimage)\nlibrary(here)\n\niroh = here(\"_posts\", \"2022-01-17-avatar-the-last-airbender-who-drinks-the-most-tea\", \"images\", \"iroh_2.png\")\n\n\n\nLoad Data from Tidy Tuesday Github Respository.\n\n\ntuesdata <- tidytuesdayR::tt_load('2020-08-11')\n\n\n\n    Downloading file 1 of 2: `avatar.csv`\n    Downloading file 2 of 2: `scene_description.csv`\n\navatar <- tuesdata$avatar\n\n\n\nWrangle Data for Tea String Search\n\n\n#search the scripts for places where characters say the word \"tea\" and keep all those observations\ntea_data <- avatar %>% \n  mutate(tea_mentions = str_count(full_text,\n                                 \" tea \")) %>% \n  group_by(character) %>% \n  summarise(tea_mentions_sum = sum(tea_mentions, na.rm = TRUE)) %>% \n  filter(tea_mentions_sum > 0) %>% \n  filter(character != \"Scene Description\")\n\nteam_mentions_max = max(tea_data$tea_mentions_sum)\n\n\n\nCreate Circular ggplot of “Tea Mentions”\n\n\n#create an index for my characters - to be used for x axis of my ggplot\ntea_data$index <- 1:nrow(tea_data)\n\n#set angles for characters based on the number of characters in my ggplot (the labels will be placed in a circle, and we want to be able to read them! )\nnumber_characters = nrow(tea_data)\nid = seq(1:number_characters)\nangle = 90 - 360 * (id) / number_characters\ntea_data$angle = ifelse(angle < -90, angle + 180, angle)\n\n#create a ggplot of \niroh_tt <- ggplot(data = tea_data) +\n  geom_segment(aes(x = index,\n                   xend = index,\n                   y = 0,\n                   yend = tea_mentions_sum),\n               color = \"cornsilk\") +\n  geom_segment(aes(x = index,\n                   xend = index,\n                   y = tea_mentions_sum,\n                   yend = team_mentions_max),\n               linetype = 'dotted',\n                 color = \"cornsilk\") +\n  geom_image(aes(x = 8.5,\n                 y = -15,\n                 image = iroh),\n             size = .25) +\n  geom_segment(x = 0,\n               xend = 17,\n               y = 0,\n               yend = 0,\n               color = \"cornsilk\") +\n   coord_polar() +\n   xlim(0, 17) +\n   ylim(-15, 35) +\n  geom_text(aes(x = index,\n                y = 30,\n                label = paste0(character, \" : \", tea_mentions_sum)),\n            angle = angle,\n            family = 'serif',\n            color = \"cornsilk\") +\n  labs(title = \"Avatar the Last Airbender: \\n Who drinks the most tea?\",\n       subtitle = \"Based on scripts from the TV series, how many times\\n does each character say the word 'tea'\\n throughout the series?\",\n       caption = \"Data from the Avatar Wiki and R for Data Science Tidy Tuesday Repository\") +\n  theme(panel.background = element_rect(\"#3D2D19\"),\n        plot.background = element_rect(fill = \"#3D2D19\"),\n        panel.grid = element_blank(),\n        axis.text = element_blank(),\n        axis.title = element_blank(),\n        axis.ticks = element_blank(),\n        text = element_text(family = 'serif',\n                            color = \"cornsilk\"),\n        plot.title = element_text(size = 16, face = 'bold', hjust = 0.5),\n        plot.subtitle = element_text(size = 10, hjust = 0.5))\n\n#save png for website\nggsave(\"iroh_tt.png\",\n       path = here(\"_posts\", \"2022-01-17-avatar-the-last-airbender-who-drinks-the-most-tea\", \"images\"))\n\n\n\n\nDistill is a publication format for scientific and technical writing, native to the web.\nLearn more about using Distill at https://rstudio.github.io/distill.\n\n\n\n",
    "preview": {},
    "last_modified": "2022-01-17T21:49:49-08:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-11-16-eds-222/",
    "title": "Seasonality and Covid-19 impacts on the National School Lunch Program",
    "description": "For my Statistics for Environmental Data Science class, I chose to explore national monthly participation in the USDA National School Lunch Program using decomposition analysis and hypothesis testing.",
    "author": [
      {
        "name": "Scout Leonard",
        "url": {
          "https://scoutcleonard.github.io": {}
        }
      }
    ],
    "date": "2021-12-12",
    "categories": [
      "class projects"
    ],
    "contents": "\nThis statistical analysis was completed for my Master’s program course, Environmental Data Science 222: Statistics for Environmental Data Science. To access the complete history of the code or submit comments of feedback as Issues, visit my project repository on Github. The code is not included in the PDF documentation of the project write-up, but is visible at this link to the live project web page.\nMy Question:\nMy analysis seeks to explore the questions: Are there seasonal differences in how U.S. school food programs feed students? What about long-term trends, and how did any trends change in 2020 during school closures from the Covid-19 pandemic?\nI have previous experience working in school food systems to expand access to this resource for school communities, but you need no time in cafeterias to guess that there would be seasonality in feeding kids. Students off for summer means fewer students participating in school lunch in July and August. I was more interested in understanding seasonal differences within the school year: for example, does the US feed more students in fall compared to winter?\nI am also interested in the impact of Covid-19 on school lunch participation. Across the U.S., many school nutrition programs adjusted to serving grab-and-go meals for students who rely on school for part of their daily nutrition during school closures. Understanding service gaps during this time will help our understanding of how effective or ineffective efforts to continue service were in feeding kids. Such knowledge can inform data-driven interventions for a future emergency school closure.\nBackground\nThe National School Lunch Program (NSLP) is an enormous food system with major implications for equity in American K-12 education. Every day, NSLP provides ~30 million children school lunch at free or reduced prices (1). It operates in public and nonprofit private schools and residential childcare facilities (2).\nTo provide meals at free and reduced cost to students, participating school districts are reimbursed subsidies for every qualifying meal they serve. To qualify for subsidy, meals served by Nutrition Services operators must meet federal meal pattern policies which define meal content.\nIn the US, an estimated 11.7 million children are estimated to live in food insecure homes this year (5). Students who experience hunger are reported to experience higher risk of decreased academic performance, more tardy and absent days, and more mental health challenges (4). A robust understanding of school food systems can create data-informed decisions around USDA and state school food policy and on-the-ground management so that students do not have hunger as an additional barrier to their wellbeing and academic achievement.\nData Description\nThe Food and Nutrition Services sector of the USDA offers monthly and annual reports of national participation in the NSLP and other school meal programs subsidized by the USDA (breakfast, seamless summer, supper, and snacks).\nTo answer my questions, I used the National Level Monthly Data for the National School Lunch Program, which gives national-level monthly participation in USDA school nutrition programs since October 2017 (47 months). This data comes in publicly available .pdf and .xlsx formats. I downloaded the .xlsx format of the monthly data and did some tidying in Excel to make sure the .csv version would be friendly with R. This version of the data can be found in my Github repository’s data folder.\n\n\n#load libraries\nlibrary(here)\nlibrary(lubridate)\nlibrary(tidyverse)\nlibrary(zoo)\nlibrary(feasts)\nlibrary(tsibble)\nlibrary(patchwork)\nlibrary(ggplot2)\nlibrary(broom)\nlibrary(gt)\n\noptions(scipen = 999)\n\n\n\n\n\n#read in the monthly lunch data \nusda_monthly <- read.csv(here(\"_posts\",\"2021-11-16-eds-222\",\"data\", \"usda_monthly_data_tidy.csv\"))\n\n\n\nEven after this manipulation in excel, there were changes to be made for easier manipulation of the monthly lunch data. All tidying and analysis uses R, which I use to give columns more descriptive names, remove odd columns that were added to the dataframe when I read in the .csv, mutate columns to exclude “%” “-” or spaces that R cannot make sense of.\n\n\n#change first column name \ncolnames(usda_monthly)[1] <- \"month\"\n\n#delete the weird columns that got added between downloading the raw data to my \n#local computer and reading it to .Rmd\nusda_monthly <- select(usda_monthly, -c(\"X\", \"X.1\"))\n\n#change last column name\ncolnames(usda_monthly)[9] <- \"fiscal_year\"\n\n\n\n\n\n#remove percent signs from percent_free_of_total_lunches and \n#percent_reduced_price_of_total_lunches columns\nusda_monthly <- usda_monthly %>% \n mutate(percent_free_of_total_lunches = gsub('%','', percent_free_of_total_lunches)) %>% \n  mutate(percent_reduced_price_of_total_lunches = gsub('%','',percent_reduced_price_of_total_lunches)) %>%\n  mutate(month = gsub(\"-\", \" \", month))\n\n\n\nLast, I make sure the class of each column is correct. All of the columns were of class numeric when read in, so I mutated the month column to yearmon using the zoo package, and everything else to numeric. I also add season and covid status columns to the data frame depending on the date of each observation for future analysis.\n\n\n#convert month column to class datetime from class character\nusda_monthly <- usda_monthly %>% \n  mutate(\"month\" = zoo::as.yearmon(month, \"%y %b\")) %>% \n  mutate(\"covid_status\" = ifelse(month > \"Feb 2020\", \"covid\", \"pre-covid\"))\n\n#convert percent free lunches to numeric\nusda_monthly <- usda_monthly %>% \n  mutate(percent_free_of_total_lunches = as.numeric(percent_free_of_total_lunches)) \n\n#add season component\nusda_monthly <- usda_monthly %>% \n  mutate(month_2 = format(month, \"%b\")) %>% \n  mutate(season = ifelse(month_2 == \"Dec\" | month_2 == \"Jan\" | month_2 == \"Feb\", \"winter\",\n                         ifelse(month_2 == \"Mar\" | month_2 == \"Apr\" | month_2 == \"May\", \"spring\",\n                                ifelse(month_2 == \"Jun\" | month_2 == \"Jul\" | month_2 == \"Aug\", \"summer\",\n                                       ifelse(month_2 == \"Sep\" | month_2 == \"Oct\" | month_2 == \"Nov\", \"fall\", NA)))))\n\n\n\nFinally, to have an initial look at my monthly lunch participation data over time, I make a simple line graph here:\n\n\n#total lunches served plot \ntotal_lunches_testplot <- ggplot(data = usda_monthly, aes(x = month, y = total_lunches_served)) +\n  geom_line(color = \"darkcyan\") +\n  theme_minimal() +\n  labs(title = \"NSLP Lunches Served (Monthly)\",\n       x = \"Month\",\n       y = \"Total Lunches Served\",\n       caption = \"Figure 1: Monthly meals served nationally by NSLP between October 2017 and August 2021\")\n\ntotal_lunches_testplot\n\n\n\n\nAt a first glance, I can see that there are seasonal dips in NSLP lunch participation in the summers, as expected. There also appears to be a dip in participation before January, which I would suspect is due to holiday recesses. It seems that the start of the school year does have higher participation than any of the winter and spring months, though. I plan to investigate this in my analysis.\nI can see that participation dropped dramatically when schools closed, which I also plan to investigate in my analysis.\nData Quality\nEvery school day, school cafeteria staff at participating NSLP schools report meals claimed by students who qualify for free- and reduced-priced lunches. That meets the numbers from this dataset are self-reported by schools and districts nationally. The USDA conducts periodically audits schools to control for accurate serving, counting, and claiming or reimbursable meals.\nThis analysis assumes that the numbers presented by the USDA are accurate monthly meals served nationally, but there is risk for systematically biased sampling. In my own work, I have observed inaccurate claim reports as a results of short staffing and reduced bandwidth for serving and counting. This may mean that school nutrition programs with lower budgets (i.e. more free-and-reduced eligible students, because of funding models) are at risk for less accurate reporting.\nAnalysis Plan\nSeasonality in NSLP\nFor the first part of my analysis, I investigate seasonal differences in NSLP participation by decomposing the data seasonal and trend components using the classical_decomposition() function and applying autoplot() to that. The classical decomposition decomposes my data into the following relationship\n\\(L{t} = S{t} + T{t} + R{t}\\)\nWhere \\(L{t}\\) is lunches served per month, \\(S{t}\\) is a seasonal component, \\(T{t}\\) is the trend component, and \\(R{t}\\) is the remainder.\nTo visualize this decomposition, I first create a tsibble with the months as class yearmonth and the total lunches served per month, then create an additive classical decomposition model. I then use this model to generate an autoplot which helps to visualize the presence of seasonality and long-term trends in the data for both groups. Finally, I generate an autocorrelation function with a lag of 12 because I want to see how much participation in one month is correlated with participation for the rest of the school year.\n\n\n#confusingly, converting the data I want in my time series requires data of class `yearmonth` not `yearmon` I learned I can use `yearmonth()` from the tsibble package to make a tsibble with the correct time class \nusda_monthly <- usda_monthly %>% \n  mutate(month = yearmonth(month))\n\nmonthly_tsib <- usda_monthly %>%\n  select(c(month, total_lunches_served)) %>% \n  as_tsibble()\n\n\n\n\n\ntotal_decomp = monthly_tsib %>% \n  model(\n    classical_decomposition(total_lunches_served, type = \"additive\")\n  ) %>% \n  components()\n\ntotal_auto <- autoplot(total_decomp) +\n  labs(title = \"Classical Additive Decomposition of Monthly Meals Served Nationally by NSLP\",\n       caption = \"Figure 3: Seasonality component has a larger impact of meals served than long term trends. \")\n\n\n\nImpact of Covid on NSLP\nIn addition to considering the trend component of the classical decomposition from March 2020 onward, I also complete a hypothesis test to compare the mean monthly participation in the NSLP before and during the Covid-19 pandemic.\n\n\ncovid_df <- usda_monthly %>% \n  filter(covid_status == \"covid\")\n\npre_covid_df <- usda_monthly %>% \n  filter(covid_status == \"pre-covid\")\n\nmean_fed_precovid <- mean(pre_covid_df$total_lunches_served)\nmean_fed_covid <- mean(covid_df$total_lunches_served)\n\n\n\nMean monthly participation in the NSLP prior to Covid-19 was 419605973 and mean monthly participation in the NSLP during the Covid-19 pandemic is 122430863.\nMy null hypothesis is that there is no significant difference between the pre- and during- Covid-19 lunches served, and my alternative hypothesis is that there is a significant difference between the conditions. I test the null hypothesis using a Welch’s two-sample t test.\nSummarize results visually and in words\nSeasonality of NSLP\n\n\n\n\n\njpeg(file = \"total_acf.jpeg\")\nplot(total_acf, main = \"Serial Correlation of National Monthly NSLP Participation\")\ndev.off()\n\n\nquartz_off_screen \n                2 \n\nThe results of the autocorrelation function (ACF) indicate that monthly participation in the NSLP for this month is positively correlated with monthly participation in the NSLP for the next two months, and the prior two months with statistical significance. Participation in all of the other months of the year have a positive correlation with the present month, but not of any statistical significance from correlation of 0, as indicated by the blue line indicating 95% confidence interval.\nI wondered if isolating participation data to only students who qualify for free lunch might change the ACF output, since I would expect more reliance on NSLP for nutrition in this population, but the ACF plot was about the same, which was interesting.\nI interpret the lack of correlation among other months as evidence that there could be more efforts to make sure students have regular access to school meals.\n\n\ntotal_auto\n\n\n\n\nThe results of my decomposition function autoplot demonstrate a stronger seasonal effect in NSLP participation among all participating students. We can see declines in participation during the summer months, as we would expect but also changes in participation during the school year. I predict that because the data is reported as a monthly total, that there are seasonal decreases in participation during months with school breaks (in winter and spring). Because of this and the domination of the summer impact to seasonality, there is not sufficient evidence enough to say that the NSLP participating schools and districts feed students differently seasonally during the school year.\nAny seasonal differences between school months were drowned out by the seasonal differences between school months and summer months. To further investigate the seasonal differences in meals served, I tried to also run hypothesis tests comparing total meals served for fall and spring months, fall and winter months, and spring and winter months, with null hypotheses that there is no difference in mean student fed between any particular season and the rest of the year.\n\n\n#t test: null hypothesis that mean difference between fall and the rest of the school year is zero\ntidy(t.test(usda_monthly$total_lunches_served[usda_monthly$season == \"fall\"], usda_monthly$total_lunches_served[usda_monthly$season != \"fall\" | usda_monthly$season != \"summer\"]))\n\n\n# A tibble: 1 × 10\n    estimate  estimate1 estimate2 statistic p.value parameter conf.low\n       <dbl>      <dbl>     <dbl>     <dbl>   <dbl>     <dbl>    <dbl>\n1 116728134. 422522363.    3.06e8      1.86  0.0820      15.9  -1.66e7\n# … with 3 more variables: conf.high <dbl>, method <chr>,\n#   alternative <chr>\n\n\n\n#t test: null hypothesis that mean difference between winter and the rest of the school year is zero\ntidy(t.test(usda_monthly$total_lunches_served[usda_monthly$season == \"winter\"], usda_monthly$total_lunches_served[usda_monthly$season != \"winter\" | usda_monthly$season != \"summer\"]))\n\n\n# A tibble: 1 × 10\n   estimate  estimate1  estimate2 statistic p.value parameter conf.low\n      <dbl>      <dbl>      <dbl>     <dbl>   <dbl>     <dbl>    <dbl>\n1 70046711. 375840940. 305794229.      1.33   0.197      21.8  -3.91e7\n# … with 3 more variables: conf.high <dbl>, method <chr>,\n#   alternative <chr>\n\n\n\n#t test: null hypothesis that mean difference between spring and the rest of the school year is zero\ntidy(t.test(usda_monthly$total_lunches_served[usda_monthly$season == \"spring\"], usda_monthly$total_lunches_served[usda_monthly$season != \"spring\" | usda_monthly$season != \"summer\"]))\n\n\n# A tibble: 1 × 10\n   estimate  estimate1  estimate2 statistic p.value parameter conf.low\n      <dbl>      <dbl>      <dbl>     <dbl>   <dbl>     <dbl>    <dbl>\n1 38540202. 344334430. 305794229.     0.634   0.534      18.1  -8.91e7\n# … with 3 more variables: conf.high <dbl>, method <chr>,\n#   alternative <chr>\n\nI show no significant differences in mean number of students fed between school year seasons. I would be curious to run this test if more data were available, as when summer data is excluded, there are only 35 observations in the sample, on the cusp of violating the Central Limit Theorem. With the data I have here, I cannot answer the question “does the the USDA NSLP show seasonality in feeding kids?” for seasons when school is in session.\nImpact of Covid on NSLP\nThe decomposition model used to understand seasonal trends in NSLP participation also indicated a strong downward trend in 2020 and 2021, which seems obviously related to school closures. As a gut check, I conducted a hypothesis test to compare the mean monthly participation in NSLP before and during the pandemic. I was not sure how to define the dates of the pandemic besides the start of nationwide lockdowns in March 2020. I considered it ongoing, since the continuing spread of the Covid-19 virus, lag in access to vaccines in students relative to adults, rules around attendance for students with symptoms or risk of interaction with infected individuals, and decisions by parents to remove students from schools are all ongoing barriers to institutions which facilitate NSLP meals. Therefore, all months including and after March 2020 are classified as “covid” in my data, and otherwise “pre-covid.”\n\n\ncovid_ttest <- t.test(total_lunches_served ~ covid_status, data = usda_monthly) \n\ncovid_pval <- round(covid_ttest$p.value, 11)\ncovid_tscore <- round(covid_ttest$statistic, 2)\ncovid_upper_ci <- round(covid_ttest$conf.int[[1]], 2)\ncovid_lower_ci <- round(covid_ttest$conf.int[[2]], 2)\n\nTTest_Component <- c(\"P Value\", \"T Score\", \"Confidence Interval Lower\", \"Confidence Interval Upper\")\nResults <- c(covid_pval, covid_tscore, covid_upper_ci, covid_lower_ci)\ncovid_df <- data.frame(TTest_Component, Results)\n\ncovid_df %>% \n  gt() %>% \n  tab_footnote(\n    footnote = \"Figure 4: Hypthesis test statistics for comparing mean students fed monthly before and during Covid.\",\n    locations = cells_column_labels(\n      columns = vars(Results)\n    )\n  )\n\n\n\nTTest_Component\n      Results1\n    P Value\n0.00000000014T Score\n-8.60000000000Confidence Interval Lower\n-367060915.12000000477Confidence Interval Upper\n-227289304.41999998689\n        \n          1\n          \n           \n          Figure 4: Hypthesis test statistics for comparing mean students fed monthly before and during Covid.\n          \n      \n    \n\nA Welch two-sample t test comparing the means of of monthly meals served before and during the pandemic had statistically significant results with a p value of 0. This means there is a mean nonzero difference between the number of kids NSLP served before and during the pandemic and there is a less than 1 percent probability of observing a difference as extreme as this if the null (0 difference) were true. The confidence interval indicates that with 95% confidence we can estimate the range -227289304.42 - -367060915.12 contains the difference in mean participation from before and during the pandemic. This is important because not only do we confirm what we imagined was true about lack of access to meals, but NSLP fed fewer children meals at a magnitude of hundreds of thousands of meals.\nNext steps and future directions\nThe USDA’s data platform only allows me to access NSLP participation data from 2017 onward. As a next step, it may be useful reach out to their data services to try to obtain data from previous years. 47 months of data are a small sample when we exclude summer data, but I wonder if a larger sample size might be better for illuminating conclusive differences in the mean monthly number of students served between school months by season, which there is no current research about.\nStudent access to NSLP is vital for supporting equity in U.S. K-12 schools and helping students to be their best, healthiest selves. Understanding gaps in feeding kids - from regular dips to national emergency program closures - can help policy-makers and on-the-ground stakeholders to make data-informed decisions that reduce identified gaps in access to school meals. Enhancing NSLP’s meals served supports millions of hungry students and their families (5). I hope that others continue to make use of the USDA’s publicly available national school meal data to continue to improve the ways we understand USDA program operations in schools.\nReferences\nhttps://www.ers.usda.gov/topics/food-nutrition-assistance/child-nutrition-programs/national-school-lunch-program/\nhttps://fns-prod.azureedge.net/sites/default/files/resource-files/NSLPFactSheet.pdf\nhttp://bestpractices.nokidhungry.org/sites/default/files/2020-06/Need%20for%20School%20Nutrition%20Budget%20Relief.pdf\nhttps://www.karger.com/Article/Pdf/66399\nhttps://www.ers.usda.gov/webdocs/publications/102076/err-298.pdf?v=3840.6\n\n\n\n",
    "preview": "posts/2021-11-16-eds-222/eds-222_files/figure-html5/unnamed-chunk-6-1.png",
    "last_modified": "2022-01-16T19:15:03-08:00",
    "input_file": {},
    "preview_width": 1152,
    "preview_height": 576
  },
  {
    "path": "posts/2021-11-12-taylor-swift/",
    "title": "Taylor Tuesday!",
    "description": "This Tidy Tuesday is from 2 months ago, but Taylor is forever.",
    "author": [
      {
        "name": "Scout Leonard",
        "url": {
          "https://scoutcleonard.github.io": {}
        }
      }
    ],
    "date": "2021-11-12",
    "categories": [
      "tidy tuesday"
    ],
    "contents": "\n\nContents\nLoad Libraries and Read in Data\nAudio Features MetaData\nColor Schemes\n\n\nWrangle audio_features data for Taylor Swift data\nData Viz and Analysis\nTakeaways and Weird Things!\nSome More Extensive Exploration!\n\n\n\nThese Tidy Tuesday analyses and visualizations come from the Tidy Tuesday Github repository for September 14, 2021. The data in the repository comes from Data.World through Sean Miller, Billboard.com and Spotify.\nI turned 13 years old in the year 2008. This is the same year Taylor Swift’s second album, Fearless was released to the world. Taylor wrote and recorded the lyrics, bursting with fairytale imagery and the highs and lows of life as a teenage girl. I was smitten. As a teenager, I came to know every lyric of every song and plastered my room in images of Swift pulled from magazines.\nI’m not at the same level of fandom as I was in 9th grade:\nHere I am at 15 years old wearing the shirt I made for the Speak Now Concert at Gilette Stadium.… but I still feel like this:\n\nwhen Taylor releases new music!\nWhen I saw that this Tidy Tuesday dataset was rich with TSwift data, I knew it would be fun for me to practice my data wrangling and visualizations with it. What follows are analyses of Taylor Swift songs by album and by audio features. It is by no means a complete blog! I found this soulmate dataset at the beginning of my data science training, and so I plan to continue to apply new concepts and skills to this ongoing blog post.\nHopefully fellow Swifty data scientists will find it as fearless, enchanted, and wonderstruck as I do!\nLoad Libraries and Read in Data\n\n\nlibrary(lubridate)\nlibrary(tidytuesdayR)\nlibrary(tidyverse)\nlibrary(skimr)\nlibrary(paletteer)\nlibrary(gt)\nlibrary(broom)\nlibrary(tayloRswift)\n\n\n\n\n\n#read in the data\ntuesdata <- tidytuesdayR::tt_load('2021-09-14')\n\n\n\n\n\naudio_features <- tuesdata$audio_features\n\n\n\nAudio Features MetaData\n\n\n#read in csv\naudio_features_metadata <- read.csv(\"audio_features_metadata.csv\")\n\n\n\nColor Schemes\nI used the Google Chrome plugin “color picker,” to pick colors from the album art of each of Taylor’s albums for use in these data visualizations. If you don’t like the color schemes, you’ll have to talk to the Swift franchise; sorry!\n\n\nalbum_cols <- c(\"#0BA5D7\", \"#D8BD7D\", \"#7D4884\", \"#AE1848\", \"#96818F\", \"#0A0A0A\", \"#FEB5DC\", \"#919191\", \"#D85F2E\")\n\n\n\nI used my album_cols palette to create this colorful table with the gt package. The variables in the audio_features dataset are confusing to understand without definitions to refer to.\n\n\naudio_features_metadata %>% \n  gt() %>% \n  tab_header(\n    title = \"Audiofeatures Dataset Variables Defined\",\n    subtitle = \"Many of the variables used in this dataset are confusing metrics, like 'danceability' - What is that? This table contains definitions  you help you navigate my analyses.\" \n  )  %>% \n  data_color(\n    columns = vars(variable),\n    colors =  scales::col_factor(\n    palette = album_cols,\n    domain = NULL)\n  )\n\n\n\nAudiofeatures Dataset Variables Defined\n    Many of the variables used in this dataset are confusing metrics, like 'danceability' - What is that? This table contains definitions  you help you navigate my analyses.\n    variable\n      class\n      description\n    song_id\ncharacter\nSong IDperformer\ncharacter\nPerformer namesong\ncharacter\nSongspotify_genre\ncharacter\nGenrespotify_track_id\ncharacter\nTrack IDspotify_track_preview_url\ncharacter\nSpotify URLspotify_track_duration_ms\ndouble\nDuration in msspotify_track_explicit\nlogical\nIs explicitspotify_track_album\ncharacter\nAlbum namedanceability\ndouble\nDanceability describes how suitable a track is for dancing based on a combination of musical elements including tempo, rhythm stability, beat strength, and overall regularity. A value of 0.0 is least danceable and 1.0 is most danceable.energy\ndouble\nEnergy is a measure from 0.0 to 1.0 and represents a perceptual measure of intensity and activity. Typically, energetic tracks feel fast, loud, and noisy. For example, death metal has high energy, while a Bach prelude scores low on the scale. Perceptual features contributing to this attribute include dynamic range, perceived loudness, timbre, onset rate, and general entropy.key\ndouble\nThe estimated overall key of the track. Integers map to pitches using standard Pitch Class notation . E.g. 0 = C, 1 = C♯/D♭, 2 = D, and so on. If no key was detected, the value is -1.loudness\ndouble\nThe overall loudness of a track in decibels (dB). Loudness values are averaged across the entire track and are useful for comparing relative loudness of tracks. Loudness is the quality of a sound that is the primary psychological correlate of physical strength (amplitude). Values typical range between -60 and 0 db.mode\ndouble\nMode indicates the modality (major or minor) of a track, the type of scale from which its melodic content is derived. Major is represented by 1 and minor is 0.speechiness\ndouble\nSpeechiness detects the presence of spoken words in a track. The more exclusively speech-like the recording (e.g. talk show, audio book, poetry), the closer to 1.0 the attribute value. Values above 0.66 describe tracks that are probably made entirely of spoken words. Values between 0.33 and 0.66 describe tracks that may contain both music and speech, either in sections or layered, including such cases as rap music. Values below 0.33 most likely represent music and other non-speech-like tracks.acousticness\ndouble\nA confidence measure from 0.0 to 1.0 of whether the track is acoustic. 1.0 represents high confidence the track is acoustic.instrumentalness\ndouble\nPredicts whether a track contains no vocals. “Ooh” and “aah” sounds are treated as instrumental in this context. Rap or spoken word tracks are clearly “vocal”. The closer the instrumentalness value is to 1.0, the greater likelihood the track contains no vocal content. Values above 0.5 are intended to represent instrumental tracks, but confidence is higher as the value approaches 1.0.liveness\ndouble\nDetects the presence of an audience in the recording. Higher liveness values represent an increased probability that the track was performed live. A value above 0.8 provides strong likelihood that the track is live.valence\ndouble\nA measure from 0.0 to 1.0 describing the musical positiveness conveyed by a track. Tracks with high valence sound more positive (e.g. happy, cheerful, euphoric), while tracks with low valence sound more negative (e.g. sad, depressed, angry).tempo\ndouble\nThe overall estimated tempo of a track in beats per minute (BPM). In musical terminology, tempo is the speed or pace of a given piece and derives directly from the average beat duration.time_signature\ndouble\nTime signaturespotify_track_popularity\ndouble\nPopularity\n\nWrangle audio_features data for Taylor Swift data\nThe audio_features dataset has contains the audio features of nearly 30,000 songs. I’m not sure how the songs were selcted, but in exploring the data, the observation include work from Louis Armstrong to Beyonce Knowles to Dolly Parton to U2. Because I am only interested in Taylor Swift’s music for this analysis, I filtered for only her music below.\nTaylor Swift has lots of oddball singles and deluxe albums featuring bonus tracks, which I thought would muddy the data by 1.) including duplicate songs from original albums and their deluxe versions and 2.) giving too many albums to compare, and albums that are not truly representative of Taylor’s various “eras.” Her Christmas album, for example, would not be interesting to compare to her main body of work, nor would her singles from the Hunger Games soundtrack. I stick to the main 9 albums here, in chronological order: Taylor Swift, Fearless, Speak Now, Red, 1989, reputation, Lover, folklore, and evermore.\nFiltering for the more recent two albums, folklore and evermore, was difficult because the formatting of the data changed between Lover and folklore. Instead of simply the album names, the spotify_track_album column began to include url-like strings of nonsense that did not resemble an album name. I used the function grepl() to specify a string within that nonsense, the actual title names, and pull the songs from those albums into my dataset. I then mutated them using str_detect() and replaced them with simply the album name!\n\n\n#create a taylor swift subset with desired albums\ntswift_data <- audio_features %>% \n  filter(performer == \"Taylor Swift\") %>% \n  filter(spotify_track_album == \"Taylor Swift\" |\n           spotify_track_album == \"Fearless\" | \n           spotify_track_album == \"Speak Now\" | \n           spotify_track_album == \"Red\" | \n           spotify_track_album == \"1989\" | \n           spotify_track_album == \"reputation\" | \n           spotify_track_album == \"Lover\" | \n           grepl(\"folklore\", spotify_track_album) | \n           grepl(\"evermore\", spotify_track_album)) \n\n#rename folklore and evermore occurances to simply the album names\ntswift_data <- tswift_data %>% \n  mutate(spotify_track_album = case_when(\n    str_detect(spotify_track_album, \"folklore\") ~ \"folklore\",\n    str_detect(spotify_track_album, \"evermore\") ~ \"evermore\",\n    TRUE ~ spotify_track_album\n  ))\n\n\n\nThen, I made a new dataframe, data_by_album, with a group_by() album name and summarize() to return mean audio features for each album. This prepared a dataset I could use to compare sound qualities across Swift’s discography.\nI also defined levels for the albums based on year of release. This way, the albums would appear in chronological order in my visualizations, which made more sense in my Swifty brain for comparing all 9 albums.\n\n\n#group by album and summarize mean audio features per album\ndata_by_album <- tswift_data %>%\n  group_by(spotify_track_album) %>% \n  summarise(mean_danceability = mean(danceability),\n            mean_energy = mean(energy),\n            mean_key = mean(key),\n            mean_loudness = mean(loudness),\n            mean_speechiness = mean(speechiness),\n            mean_acousticness = mean(acousticness),\n            mean_instrumentalness = mean(instrumentalness),\n            mean_liveness = mean(liveness),\n            mean_valence =  mean(valence),\n            mean_tempo = mean(tempo),\n            mean_popularity = mean(spotify_track_popularity))\n\n#define levels based on album chronology \ndata_by_album$spotify_track_album = factor(data_by_album$spotify_track_album,\n                                           levels = c(\"Taylor Swift\",\n                                                      \"Fearless\",\n                                                      \"Speak Now\",\n                                                      \"Red\",\n                                                      \"1989\",\n                                                     \"reputation\",\n                                                     \"Lover\",\n                                                     \"folklore\",\n                                                     \"evermore\"))\n\n\n\nData Viz and Analysis\nTakeaways and Weird Things!\nFor the non-Swifies, I am putting a summary of fast takeaways and weird observations here. I think I’ll keep playing with this data for…..ever as I grow my lil data science muscles, but here are some visualizations I wrote about. I’ll update this blog post as I work on this really fun data with more takeaways!\nOne of the most suspicious results, which made me question the data quality is the difference in loudness between Taylor’s most recent albums and all her previous ones. This was surprising considering the genre of the most recent albums. folklore and evermore are folk-inspired. Swift is quoted as describing her feelings from evermore as ones of “quiet conclusion and sort of this weird serenity.”\n\n\nloudness <- ggplot(data = data_by_album, aes(x = spotify_track_album, y = mean_loudness)) +\n  geom_histogram(stat = \"identity\", aes(fill = spotify_track_album)) +\n  theme_minimal() +\n  theme(legend.position = \"none\") +\n  labs(title = \"Loudness of Taylor Swift Albums\",\n       subtitle = \"(in the color palette of the Fearless album!!\",\n       x = \"Album Title\",\n       y = \"Mean Loudness\") +\n  scale_fill_taylor(palette = \"fearless\", guide = \"none\")\n\nloudness\n\n\n\n\nI showed a few of my friends this project and they asked about the “speechiness” audio features - doesn’t Taylor Swift speak in all of her songs? She does sing in all of her songs, but a look at the metadata tells you that “speechiness” is defined as the amount of spoken word on a track. Anything between 0.33 and 0.66 is a mix of singing and speech on a track, and anything more than 0.66 is like an audio book or spoken word. Speechiness in Taylor’s music is less than 0.33, so not speechy at all! It makes sense that reputation ranks higher relative to other albums in this feature, since it features a little rap.\n\n\nspeechiness <- ggplot(data = data_by_album, aes(x = spotify_track_album, y = mean_speechiness)) +\n  geom_histogram(stat = \"identity\", aes(fill = spotify_track_album)) +\n  theme_minimal() +\n  theme(legend.position = \"none\") +\n  labs(title = \"Speechiness of Taylor Swift Albums\",\n        subtitle = \"(in the color palette of the Speak Now album!!\",\n       x = \"Album Title\",\n       y = \"Mean Speechiness\") +\n  scale_fill_taylor(palette = \"speakNow\", guide = \"none\")\n\nspeechiness\n\n\n\n\nSome More Extensive Exploration!\nThis is all just me playing with the data. No writing, just Scout, Taylor, and some code chunks. It’s fun to practice with this dataset, so I will definitely keep working with it as a break from homework. I plan to run some linear regressions, which I’ve already started, and add interpretation. I also want to do some hypothesis testing with this data. I can tell the source for the data changed over time, so it might be interesting to see if the loudness, for example, in the most recent albums, has a statistically significant difference from previous albums.\n\n\ndanceability <- ggplot(data = data_by_album, aes(x = spotify_track_album, y = mean_danceability)) +\n  geom_histogram(stat = \"identity\", aes(fill = spotify_track_album)) +\n  theme_minimal() +\n  theme(legend.position = \"none\") +\n  labs(title = \"Danceability of Taylor Swift Albums\",\n        subtitle = \"(in the color palette of the Red album!!\",\n       x = \"Album Title\",\n       y = \"Mean Danceability\") +\n  scale_fill_taylor(palette = \"Red\", guide = \"none\")\n\ndanceability\n\n\n\n\n\n\nenergy <- ggplot(data = data_by_album, aes(x = spotify_track_album, y = mean_energy)) +\n  geom_histogram(stat = \"identity\", aes(fill = spotify_track_album)) +\n  theme_minimal() +\n  theme(legend.position = \"none\") +\n  labs(title = \"Energy of Taylor Swift Albums\",\n      subtitle = \"(in the color palette of the 1989 album!!\",\n       x = \"Album Title\",\n       y = \"Mean Energy\") +\n  scale_fill_taylor(palette = \"taylor1989\", guide = \"none\")\n\nenergy\n\n\n\n\n\n\nkey <- ggplot(data = data_by_album, aes(x = spotify_track_album, y = mean_key)) +\n  geom_histogram(stat = \"identity\", aes(fill = spotify_track_album)) +\n  theme_minimal() +\n  theme(legend.position = \"none\") +\n  labs(title = \"Key of Taylor Swift Albums\",\n       subtitle = \"(in the color palette of the Reputation album!!\",\n       x = \"Album Title\",\n       y = \"Mean Key\") +\n  scale_fill_taylor(palette = \"reputation\", guide = \"none\")\n\nkey\n\n\n\n\n\n\npopularity <- ggplot(data = data_by_album, aes(x = spotify_track_album, y = mean_popularity)) +\n  geom_histogram(stat = \"identity\", aes(fill = spotify_track_album)) +\n  theme_minimal() +\n  theme(legend.position = \"none\") +\n  labs(title = \"Popularity of Taylor Swift Albums\",\n       subtitle = \"(in the color palette of the Lover album!!\",\n       x = \"Album Title\",\n       y = \"Mean Popularity\") +\n  scale_fill_taylor(palette = \"lover\", guide = \"none\")\n\npopularity\n\n\n\n\n\n\nols_danceability <- lm(data = tswift_data, spotify_track_popularity ~ danceability)\n\nsummary(ols_danceability) %>% \n  tidy() %>% \n  gt()\n\n\n\nterm\n      estimate\n      std.error\n      statistic\n      p.value\n    (Intercept)\n63.174235\n4.754202\n13.288084\n1.069703e-20danceability\n8.136838\n7.758330\n1.048787\n2.979358e-01\n\n\n\nols_danceability_tempo <- lm(data = tswift_data, spotify_track_popularity ~ danceability + tempo + (danceability * tempo))\n\nsummary(ols_danceability_tempo) %>% \n  tidy() %>% \n  gt()\n\n\n\nterm\n      estimate\n      std.error\n      statistic\n      p.value\n    (Intercept)\n89.7606092\n19.8410224\n4.5239911\n2.545918e-05danceability\n-31.0148124\n34.8709256\n-0.8894175\n3.769611e-01tempo\n-0.2389066\n0.1785567\n-1.3379871\n1.854242e-01danceability:tempo\n0.3564445\n0.3169145\n1.1247340\n2.647151e-01\n\n\n\nggplot(data = tswift_data, aes(x = danceability, y = spotify_track_popularity)) +\n  geom_point(color = \"#AE1848\") +\n  geom_line(data = ols_danceability, aes(y = .fitted)) +\n  theme_minimal() +\n  labs(title = \"Popularity of Taylor Swift Songs by Danceability\",\n       x = \"Danceability\",\n       y = \"Spotify Track Popularity\")\n\n\n\n\n\n\nols_tempo <- lm(data = tswift_data, spotify_track_popularity ~ tempo)\n\nsummary(ols_tempo) %>% \n  tidy() %>% \n  gt()\n\n\n\nterm\n      estimate\n      std.error\n      statistic\n      p.value\n    (Intercept)\n73.37083397\n4.12193465\n17.800096\n1.704171e-27tempo\n-0.04473769\n0.03374115\n-1.325909\n1.892427e-01\n\n\n\nggplot(data = tswift_data, aes(x = tempo, y = spotify_track_popularity)) +\n  geom_point(color = \"#AE1848\") +\n  geom_line(data = ols_tempo, aes(y = .fitted)) +\n  theme_minimal() +\n  labs(title = \"Popularity of Taylor Swift Songs by Tempo\",\n       x = \"Tempo\",\n       y = \"Spotify Track Popularity\")\n\n\n\n\n\n\nsumm_tempo <- ols_tempo %>% \n  summary()\n\nsumm_tempo\n\n\n\nCall:\nlm(formula = spotify_track_popularity ~ tempo, data = tswift_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-21.0754  -5.0225   0.4435   5.2930  16.4330 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 73.37083    4.12193  17.800   <2e-16 ***\ntempo       -0.04474    0.03374  -1.326    0.189    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.103 on 69 degrees of freedom\nMultiple R-squared:  0.02485,   Adjusted R-squared:  0.01071 \nF-statistic: 1.758 on 1 and 69 DF,  p-value: 0.1892\n\n#r^2 = 0.02485\n\n\n\n\n\nols_acousticness <- lm(data = tswift_data, spotify_track_popularity ~ acousticness)\n\nsummary(ols_acousticness) %>% \n  tidy() %>% \n  gt()\n\n\n\nterm\n      estimate\n      std.error\n      statistic\n      p.value\n    (Intercept)\n65.118603\n1.419408\n45.877289\n1.970049e-53acousticness\n7.412888\n2.716302\n2.729036\n8.052126e-03\n\n\n\nggplot(data = tswift_data, aes(x = acousticness, y = spotify_track_popularity)) +\n  geom_point(color = \"#AE1848\") +\n  geom_line(data = ols_acousticness, aes(y = .fitted)) +\n  theme_minimal() +\n  labs(title = \"Popularity of Taylor Swift Songs by Acousticness\",\n       x = \"Acousticness\",\n       y = \"Spotify Track Popularity\")\n\n\n\n\n\n\nsumm_accousticness <- ols_acousticness %>% \n  summary()\n\nsumm_accousticness\n\n\n\nCall:\nlm(formula = spotify_track_popularity ~ acousticness, data = tswift_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-17.1488  -5.3399  -0.4974   4.3447  20.8125 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)    65.119      1.419  45.877  < 2e-16 ***\nacousticness    7.413      2.716   2.729  0.00805 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.796 on 69 degrees of freedom\nMultiple R-squared:  0.09742,   Adjusted R-squared:  0.08434 \nF-statistic: 7.448 on 1 and 69 DF,  p-value: 0.008052\n\n#r^2 = 0.09742\n\n\n\n\n\nols_speechiness <- lm(data = tswift_data, spotify_track_popularity ~ speechiness)\n\nsummary(ols_speechiness) %>% \n  tidy() %>% \n  gt()\n\n\n\nterm\n      estimate\n      std.error\n      statistic\n      p.value\n    (Intercept)\n65.93397\n1.231724\n53.529839\n6.260601e-58speechiness\n35.03886\n13.359345\n2.622798\n1.072452e-02\n\n\n\nsumm_speechiness <- ols_speechiness %>% \n  summary()\n\nsumm_speechiness\n\n\n\nCall:\nlm(formula = spotify_track_popularity ~ speechiness, data = tswift_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-19.1743  -5.9891   0.4157   5.3334  18.1284 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   65.934      1.232  53.530   <2e-16 ***\nspeechiness   35.039     13.359   2.623   0.0107 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.825 on 69 degrees of freedom\nMultiple R-squared:  0.09066,   Adjusted R-squared:  0.07748 \nF-statistic: 6.879 on 1 and 69 DF,  p-value: 0.01072\n\n#r^2 = 0.09066\n\n\n\n\n\nggplot(data = tswift_data, aes(x = speechiness, y = spotify_track_popularity)) +\n  geom_point(color = \"#AE1848\") +\n  geom_line(data = ols_speechiness, aes(y = .fitted)) +\n  theme_minimal() +\n  labs(title = \"Popularity of Taylor Swift Songs by Speechiness\",\n       x = \"Speechiness\",\n       y = \"Spotify Track Popularity\")\n\n\n\n\nDistill is a publication format for scientific and technical writing, native to the web.\nLearn more about using Distill at https://rstudio.github.io/distill.\n\n\n\n",
    "preview": "posts/2021-11-12-taylor-swift/swift_gif.gif",
    "last_modified": "2022-01-16T19:12:30-08:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-10-22-eds-216-meta-analysis-and-systematic-reviews/",
    "title": "EDS 216: Meta-Analysis and Systematic Reviews",
    "description": "Data aggregation and reviews and assessments of existing scientific literature!",
    "author": [
      {
        "name": "Scout Leonard",
        "url": {
          "https://scoutcleonard.github.io": {}
        }
      }
    ],
    "date": "2021-10-22",
    "categories": [
      "course reflections"
    ],
    "contents": "\nMy final course of the summer quarter was EDS 216: Meta-Analysis and Systematic Review with Professors Scott Jasechko. In the course, we began with studying existing systematic reviews of environmental data science topics. We presented one minute presentations about systematic reviews we read; a true test of understanding and fast-talking! :)\nEach of my classmates were asked to develop a question with the structure, “What is the effect of X on Y?” These investigations were to be used to develop our own systematic review protocols for the duration of the class.\nThe question I chose was based on my previous work in urban food systems in Oakland Unified School District’s Nutrition Services Program. I asked, \"What are the effects of Farm to School programs on student well-being. My systematic review protocol is in the PDF below.\n\n\n\nOne of the main skills I learned in this class was a protocol for conducting literature searches by creating search strings from defined PICO terms. PICO terms represent the Population, Intervention, Comparative, and Outcome of your question of interest. Using Boolean operators and wildcards, you can conduct searches for literature that meets the structure of your scientific question.\nSystematic reviews are vitally important. Science is expensive, and knowing if and how your question has been answered can prevent duplicate research or inform better, further exploration. Also, scientific literature is massively complex; systematic reviews can be helpful briefs for policy-makers to understand a body of literature without having to read it all themselves!\nDistill is a publication format for scientific and technical writing, native to the web.\nLearn more about using Distill at https://rstudio.github.io/distill.\n\n\n\n",
    "preview": "posts/2021-10-22-eds-216-meta-analysis-and-systematic-reviews/files/systematic_review.jpg",
    "last_modified": "2022-01-16T19:07:09-08:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-09-05-eds-215-introduction-to-data-storage-and-management/",
    "title": "EDS 215: Introduction to Data Storage and Management",
    "description": "A reflection of my graduate school course, EDS 215.",
    "author": [
      {
        "name": "Scout Leonard",
        "url": {}
      }
    ],
    "date": "2021-09-05",
    "categories": [
      "course reflections"
    ],
    "contents": "\nThis week’s class was EDS 215: Introduction to Data Storage and Management with James Frew.\nMost of our work this week was conducted in the command line of my computer’s terminal. This was a little more scary than the last three weeks of R, because running commands in the terminal does not always return visual results when “something has happened,” good or bad! The terminal is an efficient way to scale processes that might take more time manually in the graphic user interface (GUI) of your computer. For example, if you want to rename 30 files in your computer, you can use commands typed into your terminal (this is relevant for Mac users, like me), which uses the Shell to process those commands. This process can rename the files by some convention using only a bit of code! So powerful, but with great power….\nA concept that Frew emphasized is the use of scripts in bash. Writing command line commands and codes into a script like Text Edit helps use to write reproducible code, because we cannot save codes run in the Terminal like in R. We can also run the code saved in a script straight from the script in the terminal.\nA ~ thing ~ I learned this week is that you can erase all the data on your computer in the terminal, which makes me feel like never touching it. So. Much. Power.\nOverall, this class was the most tough for me so far! It was a lot like learning a new language, and I plan to review the relevant Software Carpentry texts to make sure I’ve really grasped these concepts.\nDistill is a publication format for scientific and technical writing, native to the web.\nLearn more about using Distill at https://rstudio.github.io/distill.\n\n\n\n",
    "preview": "posts/2021-09-05-eds-215-introduction-to-data-storage-and-management/images/image.png",
    "last_modified": "2021-09-29T13:41:32-07:00",
    "input_file": {},
    "preview_width": 266,
    "preview_height": 442
  },
  {
    "path": "posts/2021-09-04-eds-214-analytical-workflows-and-scientific-reproducibility/",
    "title": "EDS 214: Analytical Workflows and Scientific Reproducibility",
    "description": "A reflection of my graduate school course, EDS 214.",
    "author": [
      {
        "name": "Scout Leonard",
        "url": {
          "https://scoutcleonard.github.io": {}
        }
      }
    ],
    "date": "2021-09-04",
    "categories": [
      "course reflections"
    ],
    "contents": "\nAnother week done!\nMy brain was worked in totally different ways in Environmental Data Science 214! Unlike in the last 3 weeks of course work, the emphasis was less on technical aspects of using R and more on theoretical concepts of collaborating with your future self and others!\nA concept we learned in EDS 214 is a workflow known as pair programming. Sometimes, when you are working on a project with others, you may code separately on different computers on different parts of the project and different chucks of code. Other times, you may “pair program” by working with one or multiple partners on one computer. In this case, one person is writing code and the rest of the team focuses on the solutions to whatever problem the team wants to solve. The navigator participates by discussing with the driver and other natigators possible solutions, or researching possible solutions. These roles have names: the typing team member is know as the driver and their teammates are known as navigators.\nA ~ thing ~ I have learned about is external servers! Our class has one named Taylor, after the incredible environmental justice scholar from Yale University’s School of the Environment, Doreceta Taylor. When we work with larger datasets in R, our computers will not be able to handle the data as they have so far in our R Projects housed on our machines. Instead, we can access data for projects by storing them on the server. Taylor can only be accessed through UCSB Wifi, or else through the university VPN. I have a personal drive there, as well as access to class drives for coursework which will use storage on the server.\nDistill is a publication format for scientific and technical writing, native to the web.\nLearn more about using Distill at https://rstudio.github.io/distill.\n\n\n\n",
    "preview": "posts/2021-09-04-eds-214-analytical-workflows-and-scientific-reproducibility/Unknown.png",
    "last_modified": "2021-09-29T13:50:31-07:00",
    "input_file": {},
    "preview_width": 1824,
    "preview_height": 391
  },
  {
    "path": "posts/2021-08-16-eds-221-things-im-learning/",
    "title": "EDS 221: Things I'm Learning!",
    "description": "A reflection on all the things I'm learning in my first weeks of graduate school.",
    "author": [
      {
        "name": "Scout Leonard",
        "url": {
          "https://scoutcleonard.github.io": {}
        }
      }
    ],
    "date": "2021-08-16",
    "categories": [
      "course reflections"
    ],
    "contents": "\nI’m learning lots in Environmental Data Science 221: Scientific Programming Essentials!\nHere are some examples of things I’ve learned so far:\nOne function we’ve learned that I enjoy is the case_when() function. When paired with the mutate() function, we can sort data in a newly created column based on existing data. Below is an example with the Palmer Penguins dataset.\n\n\n\nI made this stacked bar graph so you can see that I have a new column that has two classifications for flipper length: medium and small.\nA concept we’ve learned in EDS 221 is tidy data. As an organizational fiend, this has been one of my favorite concepts! I have worked on many, many untidy spreadsheets over the years. In tidy data, data is organized in a predictable way! Most importantly, tidy data has:\n1.) Each variable is a column. 2.) Each observation is a row. 3.) Each cell contains 1 value.\nA thing I have learned about doing data science is that reproducible workflows are essential, and acheived in various ways! Valuing the code over the product (i.e. saving and pushing to repo) is more important that the output (html you knit!). Create a repo and stage, commit, push, pull early and often! Try not to copy and paste code. Be aware of the order in which you put functions. Do not alter your original data set. There are many more, but as you can see, reproducible workflows are essential.\nDistill is a publication format for scientific and technical writing, native to the web.\nLearn more about using Distill at https://rstudio.github.io/distill.\n\n\n\n",
    "preview": "posts/2021-08-16-eds-221-things-im-learning/images/meds_photo.jpg",
    "last_modified": "2021-09-29T13:50:58-07:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-08-10-hello/",
    "title": "My First Blog Post!",
    "description": "Hello & welcome to my blog.",
    "author": [
      {
        "name": "Scout Leonard",
        "url": {}
      }
    ],
    "date": "2021-08-10",
    "categories": [],
    "contents": "\nHi there and welcome!\nIf you’re reading this, it means you’ve found my blog. I am currently learning to create my own website in R, so thanks for your patience as this masterpiece takes shape.\nI plan to use this blog page as a way to record just a tiny fraction of the amazing amount of things I am learning in my Master’s program. Mainly, it is a way for me to reflect on the work I’ve done and to improve and reproduce my code. If you find it interesting or helpful, that is a happy plus :)\nWarm regards and happy coding,\nScout\nDistill is a publication format for scientific and technical writing, native to the web.\nLearn more about using Distill at https://rstudio.github.io/distill.\n\n\n\n",
    "preview": {},
    "last_modified": "2022-01-16T19:07:32-08:00",
    "input_file": {}
  }
]
